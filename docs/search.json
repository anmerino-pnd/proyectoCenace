[
  {
    "objectID": "6_documentacion.html",
    "href": "6_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\n\n\n\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd proyectoCenace\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Conexión a la base de datos SQL\nip=\nport=\nuser=\npwd=\ndb=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nurl= '' \nToken-api=''\nToken-ct=''\nContent-Type=''\nCookie=''\n\ndominio=\"\"\nboundary=''\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_COLLECTION_SESSIONS = \"\"\nMONGO_COLLECTION_MESSAGE_BACKUP = \"\"\nMONGO_COLLECTION_PRODUCTS = \"\"\nMONGO_COLLECTION_SALES = \"\"         \nMONGO_COLLECTION_SPECIFICATIONS = \"\"\n\nLevantar el backend con Uvicorn:\n\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\n\n\n\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd proyectoCenace\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Conexión a la base de datos SQL\nip=\nport=\nuser=\npwd=\ndb=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nurl= '' \nToken-api=''\nToken-ct=''\nContent-Type=''\nCookie=''\n\ndominio=\"\"\nboundary=''\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_COLLECTION_SESSIONS = \"\"\nMONGO_COLLECTION_MESSAGE_BACKUP = \"\"\nMONGO_COLLECTION_PRODUCTS = \"\"\nMONGO_COLLECTION_SALES = \"\"         \nMONGO_COLLECTION_SPECIFICATIONS = \"\"\n\nLevantar el backend con Uvicorn:\n\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#documentación-técnica-del-código",
    "href": "6_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2. Documentación técnica del código",
    "text": "2. Documentación técnica del código\n\n2.1. Estructura de carpetas y módulos\n\n\n2.2. Modelos LLM utilizados\n\n\n2.3. Puntos de entrada y funciones clave"
  },
  {
    "objectID": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3. Guía de entrenamiento y mejora",
    "text": "3. Guía de entrenamiento y mejora\n\n3.1. Generación de la base de datos vectorial\n\n\n3.2. Flujo de la interacción"
  },
  {
    "objectID": "6_documentacion.html#diagrama-de-arquitectura",
    "href": "6_documentacion.html#diagrama-de-arquitectura",
    "title": "Documentación",
    "section": "4. Diagrama de arquitectura",
    "text": "4. Diagrama de arquitectura"
  },
  {
    "objectID": "4_modelado.html",
    "href": "4_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "Dado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\nEl sistema se basa en una arquitectura de Recuperación Aumentada por Generación (RAG). Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\nLa consulta se convierte en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario."
  },
  {
    "objectID": "4_modelado.html#modelado",
    "href": "4_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "Dado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\nEl sistema se basa en una arquitectura de Recuperación Aumentada por Generación (RAG). Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\nLa consulta se convierte en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario."
  },
  {
    "objectID": "4_modelado.html#evaluación",
    "href": "4_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa fase de evaluación es crucial para validar el desempeño del sistema y asegurar que cumple con los objetivos del proyecto. La evaluación se realiza a través de pruebas manuales y automatizadas.\n\n2.1 Criterios de evaluación"
  },
  {
    "objectID": "2_comprension_datos.html",
    "href": "2_comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nEn una primera instancia, se realizó una Exploración de Datos (EDA) sobre un conjunto de 175 incidentes correspondientes al período de enero de 2023 a mayo de 2024. Este conjunto incluía 14 variables, como título, descripción, categoría, solución, entre otras. Se utilizará el título y la descripción para sugerir la categoría de un incidente.\nAunque se obtuvieron hallazgos interesantes, este EDA evidenció la necesidad de hacer un análisis con más datos y abordar inconsistencias presentes en los registros analizados.\n\n\n\nPara ampliar la cantidad de datos disponibles, fue necesario realizar un proceso de codificación y anonimización. Esto fue para garantizar la protección de información sensible, como nombres de personal, números telefónicos, correos electrónicos, normas, y nombres de subestaciones, entre otros.\nCon herramientas como expresiones regulares y SpaCy, se logró recuperar un total de 2,817 registros, es decir, 15 veces más registros que los disponibles al inicio."
  },
  {
    "objectID": "2_comprension_datos.html#recolección-de-los-datos",
    "href": "2_comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nEn una primera instancia, se realizó una Exploración de Datos (EDA) sobre un conjunto de 175 incidentes correspondientes al período de enero de 2023 a mayo de 2024. Este conjunto incluía 14 variables, como título, descripción, categoría, solución, entre otras. Se utilizará el título y la descripción para sugerir la categoría de un incidente.\nAunque se obtuvieron hallazgos interesantes, este EDA evidenció la necesidad de hacer un análisis con más datos y abordar inconsistencias presentes en los registros analizados.\n\n\n\nPara ampliar la cantidad de datos disponibles, fue necesario realizar un proceso de codificación y anonimización. Esto fue para garantizar la protección de información sensible, como nombres de personal, números telefónicos, correos electrónicos, normas, y nombres de subestaciones, entre otros.\nCon herramientas como expresiones regulares y SpaCy, se logró recuperar un total de 2,817 registros, es decir, 15 veces más registros que los disponibles al inicio."
  },
  {
    "objectID": "2_comprension_datos.html#descripción-de-los-datos",
    "href": "2_comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2. Descripción de los datos",
    "text": "0.2. Descripción de los datos\nA partir de estos últimos datos extraídos, obtenemos las siguiente información:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2817 entries, 0 to 2816\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   titulo       2817 non-null   object\n 1   descripcion  2817 non-null   object\n 2   solucion     1159 non-null   object\n 3   categories   2817 non-null   object\n 4   fecha        2817 non-null   object\ndtypes: object(5)\nmemory usage: 110.2+ KB\nDonde cada variable tiene la siguiente descripción:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\ntitulo\nTítulo del incidente en cuestión.\nTexto\n\n\ndescripcion\nDesarrollo de la problemática y explicación del incidente.\nTexto\n\n\nsolucion\nExplicación de cómo se llegó a la solución.\nTexto\n\n\ncategories\nCategoría a la que pertenece la problemática.\nTexto\n\n\nfecha\nFecha.\nTexto"
  },
  {
    "objectID": "2_comprension_datos.html#exploración-de-los-datos",
    "href": "2_comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de categorías\nEmpezamos la exploración analizando la cantidad de categorías únicas que teníamos en los tickets. Ya que parte del trabajo es la creación de un modelo capaz de clasificar, o asignar la categoría de tickets a partir de su contenido.\ncategories\nADTR SP7 &gt; SCADA                                     1410\nINTRANET Y SOPORTE DE APLICACIONES                    379\nCOMPUTO Y PERIFERICOS                                 257\nSEGURIDAD INFORMATICA                                 166\nCORREO ELECTRONICO                                     97\nADTR SP7 &gt; SIREL                                       89\nADTR SP7                                               82\nTELEFONIA Y HERRAMIENTAS COLABORATIVAS                 55\nINFRAESTRUCTURA BASICA Y DE SERVICIOS PROPIOS          51\nADTR &gt; Consulta                                        38\nADTR SP7 &gt; Historico                                   37\nADTR SP7 &gt; SIGUARD                                     32\nINTERNET                                               32\nADTR SP7 &gt; Hospedaje                                   23\nADOMEM                                                 16\nADTR                                                   11\nDESARROLLO DE APLICACIONES                             10\nOPERACION DE RED DE DATOS                               9\nMESA DE SERVICIO                                        8\nBASE DE DATOS                                           7\nADTR &gt; Hospedaje de Aplicativos de Potencia (EMS)       6\nMONITOREO DE ACTIVOS DE TIC                             2\nName: count, dtype: int64\nObservando los valores, tomamos la decisión de partir solamente con tickets que tengan más de 20 incidentes por categoría, esta decisión fue pensada en la cantidad de datos necesarios para entrenamiento, validación y testeo. Teniendo finalmente un total de 14 categorías.\n\n\n0.3.2. Distribución de las palabras\nEn esta sección, analizaremos la cantidad de palabras utilizadas en los títulos, descripciones y soluciones. Lo que nos permite darnos una idea de la cantidad de información que disponemos y la cual puede servir para responder a problemáticas que ya se han trabajado anteriormente, además de la creación de un modelo que pueda clasificar tickets a partir de títulos y descripciones.\n\n\n\nDistribución de palabras en los títulos\n\n\n\n\n\nDistribución de palabras en las descripciones\n\n\n\n\n\nDistribución de palabras en las soluciones\n\n\n\n\n0.3.3. Análisis de los bigramas más comunes\n\n\n\nBigramas de los títulos\n\n\n\n\n\nBigramas de las descripciones"
  },
  {
    "objectID": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes"
  },
  {
    "objectID": "0_home.html",
    "href": "0_home.html",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "0_home.html#introducción",
    "href": "0_home.html#introducción",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "1_comprension.html",
    "href": "1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "El Centro Nacional de Control de Energía (CENACE) es el organismo encargado de la planeación y el control operativo del Sistema Eléctrico Nacional (SEN). El CENACE cuenta con una mesa de ayuda (Help Desk), la cual provee soporte y le da seguimiento a incidentes reportados en todo el país. En este trabajo nos restringimos a los incidentes reportados para la zona noroeste, la cual se conforma de los estados de Sonora y Sinaloa. Los incidentes se reportan en formato de tickets y son gestionados por los ingenieros de la organización.\nTeniendo en cuenta la naturaleza crítica del sector energético, la constante evolución tecnológica y la gran cantidad de documentación técnica existente, surge la necesidad de optimizar el acceso a la información. La búsqueda manual de esta información para resolver incidentes puede ser un proceso lento, impactando la eficiencia operativa. Además de aprovechar la base de conocimientos que ya se tiene para proponer soluciones a problemas previamente vistos."
  },
  {
    "objectID": "1_comprension.html#propuesta-de-solución",
    "href": "1_comprension.html#propuesta-de-solución",
    "title": "Comprensión del negocio",
    "section": "0.1. Propuesta de solución",
    "text": "0.1. Propuesta de solución\nProponemos desarrollar un sistema de Help Desk inteligente basado en Inteligencia Artificial Generativa que utilice la metodología de Recuperación Aumentada por Generación (RAG) y modelos de lenguaje grandes (LLM). Este sistema actuará como una herramienta de apoyo para los ingenieros, proporcionando respuestas inmediatas a sus consultas técnicas. El sistema permitirá una clasificación y recuperación de información más eficiente, y generará respuestas contextualizadas a partir de la base de conocimientos interna del CENACE."
  },
  {
    "objectID": "1_comprension.html#objetivos",
    "href": "1_comprension.html#objetivos",
    "title": "Comprensión del negocio",
    "section": "0.2. Objetivos",
    "text": "0.2. Objetivos\nEl objetivo principal es elaborar un sistema de Help Desk que utilice la base de conocimientos del CENACE y modelos de lenguaje grande para que los ingenieros tengan apoyo inmediato y puedan tomar decisiones rápidas al alcance de la mano. Con esto, se espera ahorrar tiempo en la clasificación, recuperación y generación de la información técnica, y a su vez, nutrir la base de conocimientos con las soluciones generadas."
  },
  {
    "objectID": "1_comprension.html#terminología",
    "href": "1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3. Terminología",
    "text": "0.3. Terminología\n\nRAG (Retrieval-Augmented Generation): Un enfoque de IA que combina la recuperación de información con la generación de lenguaje, para crear respuestas más precisas y contextualizadas.\nLLM (Large Language Model): Modelo de lenguaje grande capaz de comprender y generar texto similar al humano, como el modelo gemma3:4b que se utilizará en el proyecto.\nOllama: Un framework que permite ejecutar modelos de lenguaje grandes de código abierto de forma local.\nVector Embeddings: Representaciones numéricas de texto que capturan su significado semántico, facilitando la búsqueda de información similar.\nBase de datos vectorial: Una base de datos optimizada para almacenar y buscar vector embeddings.\nFastAPI: Un framework web de Python de alto rendimiento para construir APIs.\nMongoDB: Una base de datos NoSQL que se utilizará para almacenar el historial de conversaciones, tickets y la documentación original."
  },
  {
    "objectID": "1_comprension.html#beneficios",
    "href": "1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4. Beneficios",
    "text": "0.4. Beneficios\n\nInnovación en el soporte técnico: Introducir un nuevo enfoque para acceder a la información técnica, brindando una experiencia más personalizada y eficiente para los ingenieros.\nOptimización del flujo de trabajo: El sistema permitirá a los ingenieros ahorrar tiempo en la búsqueda de información, lo que se traducirá en una mayor eficiencia operativa y una toma de decisiones más rápida.\nPreservación del conocimiento: El sistema ayuda a estructurar y hacer accesible la vasta base de conocimientos del CENACE, garantizando que el conocimiento institucional no se pierda.\nClasificación y sugerencia automática: El sistema puede ayudar a clasificar los tickets de incidentes y sugerir soluciones, lo que agiliza el proceso de resolución."
  },
  {
    "objectID": "1_comprension.html#costos",
    "href": "1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5. Costos",
    "text": "0.5. Costos\n\nTiempo: El proyecto tiene un plazo estimado para desarrollar una versión funcional que pueda ser evaluada y mejorada.\nFinancieros: Aunque se utilizan modelos y herramientas de código abierto, se consideran costos asociados al hardware necesario para ejecutar los modelos de manera local (servidores, etc.)."
  },
  {
    "objectID": "5_despliegue.html#plan-de-implementación",
    "href": "5_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de implementación",
    "text": "2. Plan de implementación"
  }
]