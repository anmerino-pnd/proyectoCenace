[
  {
    "objectID": "6_documentacion.html",
    "href": "6_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\n\n\n\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nLevantar el backend con Uvicorn:\n\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\n\n\n\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nLevantar el backend con Uvicorn:\n\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#documentación-técnica-del-código",
    "href": "6_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2. Documentación técnica del código",
    "text": "2. Documentación técnica del código\nLa solución se basa en una arquitectura de Recuperación Aumentada con Generación (RAG). La estructura modular del código, organizada en paquetes de Python, permite una clara separación de responsabilidades.\n\n2.1. Estructura de carpetas y módulos\n\nAPI/\n\nchat.py: Contiene los endpoints de FastAPI para interactuar con el chatbot.\nmain.py: Archivo principal que define la aplicación FastAPI y monta los endpoints.\n\nollama/\n\nassistant.py: Clase que encapsula la lógica para generar embeddings utilizando el modelo bge-m3:latest de Ollama.\n\ndoccollection.py: Módulo que maneja la carga y el procesamiento de documentos (PDF’s) para generar fragmentos de texto.\nrag.py: Clase principal del sistema RAG que integra el assistant, el doccollection y el vectorstore.\nvectorstore.py: Módulo que implementa la base de datos vectorial con FAISS.\nsettings/\n\nclients.py: Archivo de configuración que establece la conexión con la base de datos y el cliente de Ollama.\nconfig.py: Define las rutas de directorios para los vectores y los documentos procesados.\n\ntools/\n\nassistant.py: Clase base abstracta para el asistente LLM.\ndoccollection.py: Clase base abstracta para la colección de documentos.\nembedder.py: Clase base abstracta para el generador de embeddings.\nvectorstore.py: Clase base abstracta para almacén de vectores.\n\ntypes.py: Módulo que define modelos de datos con Pydantic para tipado de datos como Text, TextMetadata, Question, etc.\n\n\n\n2.2. Modelos LLM utilizados\nEl flujo de información en el sistema RAG sigue dos rutas principales:\n\nIndexación de documentos:\n\n\nLos archivos PDF son cargados y procesados por el módulo doccollection.py.\ndoccollection divide cada documento en fragmentos.\nCada fragmento es enviado al embedder.py para generar su representación vectorial.\nLos vectores resultantes se almacenan en la base de datos vectorial de FAISS, implementada en vectorstore.py, junto con sus metadatos.\n\n\nProceso de consulta (QA):\n\n\nUna consulta de usuario llega el endpoint de chat.py.\nLa consulta es vectorizada por el embedder.\nEl vectorstore realiza una búsqueda de similitud semántica para recuperar los fragmentos de documento más relevantes.\nEstos fragmentos se envían al assistant.py, que los utiliza como contexto.\nEl assistant utiliza el LLM (gemma3:4b) para generar una respuesta coherente y contextualizada.\nLa respuesta es devuelta al usuario a través del chat.py y el main.py.\n\n\n\n2.3. Puntos de entrada y funciones clave\n\nGestión de conversaciones: El módulo assistant.py gestiona el historial de conversación en MongoDB, permitiendo que el chatbot mantenga un contexto limitado con el usuario.\nGestión de tickets: Las funciones add_ticket y update_ticket_metadata en rag.py y sus respectivos endpoints en chat.py demuestran la capacidad del sistema para interactuar y actualizar una base de datos de tickets.\nBucle de retroalimentación: La funcionalidad has_liked_solution_in_conversation permite identificar y potencialmente re-indexar soluciones validadas por los usuarios, mejorando continuamente la base de conocimientos."
  },
  {
    "objectID": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3. Guía de entrenamiento y mejora",
    "text": "3. Guía de entrenamiento y mejora\n\n3.1. Generación de la base de datos vectorial\n\n\n3.2. Flujo de la interacción"
  },
  {
    "objectID": "6_documentacion.html#arquitectura-del-sistema",
    "href": "6_documentacion.html#arquitectura-del-sistema",
    "title": "Documentación",
    "section": "4. Arquitectura del sistema",
    "text": "4. Arquitectura del sistema\nEl siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial.\n\n\n\nArquitectura del sistema\n\n\n\n4.1. Componentes clave de conversación y chat\n\nPOST /chat/stream: Endpoint principal para la interacción conversacional. Recibe una consulta y un conversation_id, y devuelve una respuesta generada por el LLM en tiempo real a través de un stream.\nGET /chat/history/{user_id}/{conversation_id}: Recupera el historial de mensajes de una conversación específica.\nPOST /conversations: Crea una nueva conversación, generando un conversation_id único.\nGET /conversations/{user_id}: Lista todas las conversaciones de un usuario, incluyendo sus títulos y la fecha de la última actualización.\nDELETE /conversations: Elimina una conversación específica y su historial de la base de datos.\nPATCH /message-metadata: Permite actualizar los metadatos de un mensaje, utilizado para la funcionalidad de “gustar” una solución.\n\n\n\n4.2. Componentes clave de documentos y soluciones\n\nPOST /documents: Permite cargar nuevos archivos (en formato PDF) a la base de datos vectorial para expandir la base de conocimientos.\nGET /documents: Lista todos los documentos que han sido procesados y están disponibles para la consulta.\nDELETE /documents: Elimina un documento específico de la base de datos vectorial, eliminando también su referencia y los fragmentos asociados.\nPOST /solutions: Procesa y re-indexa soluciones “gustadas” por los usuarios, agregándolas como nuevos documentos a la base de datos vectorial para mejorar la precisión del sistema.\nDELETE /solutions: Elimina una solución específica de la base de datos vectorial.\n\n\n\n4.3. Componentes clave de tickets\n\nGET /tickets: Recupera una lista de todos los tickets almacenados en la base de datos de MongoDB.\nPOST /tickets: Permite añadir un nuevo ticket a la base de datos, con campos como título, descripción y categoría.\nPATCH /tickets/{ticket_reference}: Actualiza los metadatos de un ticket existente, como su estado de solución (is_solved)."
  },
  {
    "objectID": "4_modelado.html",
    "href": "4_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "4_modelado.html#modelado",
    "href": "4_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "4_modelado.html#evaluación",
    "href": "4_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa fase de evaluación es crucial para validar el desempeño del sistema y asegurar que cumple con los objetivos del proyecto. La evaluación se realiza a través de pruebas manuales y automatizadas.\n\n2.1 Criterios de evaluación"
  },
  {
    "objectID": "2_comprension_datos.html",
    "href": "2_comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "2_comprension_datos.html#recolección-de-los-datos",
    "href": "2_comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "2_comprension_datos.html#descripción-de-los-datos",
    "href": "2_comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2. Descripción de los datos",
    "text": "0.2. Descripción de los datos\nA partir de los datos extraídos, se obtuvo la siguiente información:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2817 entries, 0 to 2816\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   titulo       2817 non-null   object\n 1   descripcion  2817 non-null   object\n 2   solucion     1159 non-null   object\n 3   categories   2817 non-null   object\n 4   fecha        2817 non-null   object\ndtypes: object(5)\nmemory usage: 110.2+ KB\nLas variables se describen a continuación:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\ntitulo\nTítulo del incidente en cuestión.\nTexto\n\n\ndescripcion\nDesarrollo de la problemática y explicación del incidente.\nTexto\n\n\nsolucion\nExplicación de cómo se llegó a la solución.\nTexto\n\n\ncategories\nCategoría a la que pertenece la problemática.\nTexto\n\n\nfecha\nFecha.\nTexto"
  },
  {
    "objectID": "2_comprension_datos.html#exploración-de-los-datos",
    "href": "2_comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de categorías\nLa exploración comenzó con el análisis del número de categorías únicas en los tickets. Esto es crucial, ya que parte del proyecto es desarrollar un modelo capaz de clasificar o asignar una categoría a nuevos tickets a partir de su contenido.\ncategories\nADTR SP7 &gt; SCADA                                     1410\nINTRANET Y SOPORTE DE APLICACIONES                    379\nCOMPUTO Y PERIFERICOS                                 257\nSEGURIDAD INFORMATICA                                 166\nCORREO ELECTRONICO                                     97\nADTR SP7 &gt; SIREL                                       89\nADTR SP7                                               82\nTELEFONIA Y HERRAMIENTAS COLABORATIVAS                 55\nINFRAESTRUCTURA BASICA Y DE SERVICIOS PROPIOS          51\nADTR &gt; Consulta                                        38\nADTR SP7 &gt; Historico                                   37\nADTR SP7 &gt; SIGUARD                                     32\nINTERNET                                               32\nADTR SP7 &gt; Hospedaje                                   23\nADOMEM                                                 16\nADTR                                                   11\nDESARROLLO DE APLICACIONES                             10\nOPERACION DE RED DE DATOS                               9\nMESA DE SERVICIO                                        8\nBASE DE DATOS                                           7\nADTR &gt; Hospedaje de Aplicativos de Potencia (EMS)       6\nMONITOREO DE ACTIVOS DE TIC                             2\nName: count, dtype: int64\nAl observar la distribución, se decidió trabajar únicamente con categorías que tuvieran más de 20 incidentes, considerando la cantidad de datos necesaria para los procesos de entrenamiento, validación y prueba. Esta selección resultó en un total de 14 categorías.\n\n\n0.3.2. Distribución de las palabras\nEn esta sección se analiza la cantidad de palabras utilizadas en los títulos, descripciones y soluciones. Esto nos permite entender la cantidad de información disponible que puede ser útil para resolver problemas recurrentes y para el desarrollo del modelo de clasificación.\n\n\n\nDistribución de palabras en los títulos\n\n\nA partir de esta gráfica podemos notar que alrededor de 9 palabras promedio son las que se utilizan en los títulos; lo cual es normal dado que tendemos a englobar las problemáticas en pocas palabras. Sin embargo, vemos que hay problemáticas que pueden pasar el promedio, hasta llegar a las 30 palabras aproximadamente.\n\n\n\nDistribución de palabras en las descripciones\n\n\nPara las descripciones vemos que el promedio es mayor, aproximadamente hasta las 88 palabras, aunque en la mayoría de los casos son menos las que se utilizan. Esto también es normal ya que aquí es donde las personas desarrollan los detalles del incidente el cual están enfrentando. También vemos que hay incidentes que pueden tomar tantas palabras hasta llegar a las 400, 500, y hasta las 1000, aunque este último sea poco común.\n\n\n\nDistribución de palabras en las soluciones\n\n\nEn este caso, nosotros esperábamos que en esta sección hubiéramos encontrado una mayor cantidad de palabras, porque en este caso encontramos que en promedio se utilizan 23 palabras, donde frecuentemente son menos. También encontramos que no todos los incidentes contienen una descripción detallada de la solución o de los pasos que se siguieron para resolver la situación; vimos que solo el 41% de los incidentes contienen una explicación de la solución.\n\n\n0.3.3. Análisis de los bigramas más comunes\nSe realizó un análisis de los pares de palabras más frecuentes en los títulos y descripciones para identificar patrones en la forma en que se plantean las problemáticas.\n\n\n\nBigramas de los títulos\n\n\nEn los títulos lo primero que llama la atención es la codificación de ciertas palabras, ya que por detalles confidenciales, se codificaron nombres propios, sistemas, subestaciones, etc. Luego, podemos ver que se mencionan muchas veces los despliegues de distintos sistemas de software y actualizaciones.\n\n\n\nBigramas de las descripciones\n\n\nEn las descripciones vemos que en la gran mayoría de los casos son saludos hacia la persona que se están dirigiendo. Dejando de lado estos saludos, vemos que se mencionan detalles muy técnicos y específicos que solo expertos en el tema podrán entender."
  },
  {
    "objectID": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nA partir de los resultados previos, se determinó que la columna con menos información es la de soluciones. Solo el 41% de los registros contienen una explicación, con un promedio de 23 palabras, aunque en muchos casos la cantidad es menor.\nAl discutir esta situación con los expertos, se descubrió que el desarrollo y la documentación de soluciones no es una práctica común en el departamento, lo que resulta en una pérdida de conocimiento. Esto motivó la búsqueda de una propuesta para mejorar la persistencia de las soluciones."
  },
  {
    "objectID": "0_home.html",
    "href": "0_home.html",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "0_home.html#introducción",
    "href": "0_home.html#introducción",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "1_comprension.html",
    "href": "1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "El Centro Nacional de Control de Energía (CENACE) es el organismo encargado de la planeación y el control operativo del Sistema Eléctrico Nacional (SEN). El CENACE cuenta con una mesa de ayuda (Help Desk), la cual provee soporte y le da seguimiento a incidentes reportados en todo el país. En este trabajo nos restringimos a los incidentes reportados para la zona noroeste, la cual se conforma de los estados de Sonora y Sinaloa. Los incidentes se reportan en formato de tickets y son gestionados por los ingenieros de la organización.\nTeniendo en cuenta la naturaleza crítica del sector energético, la constante evolución tecnológica y la gran cantidad de documentación técnica existente, surge la necesidad de optimizar el acceso a la información. La búsqueda manual de esta información para resolver incidentes puede ser un proceso lento, impactando la eficiencia operativa. Además de aprovechar la base de conocimientos que ya se tiene para proponer soluciones a problemas previamente vistos."
  },
  {
    "objectID": "1_comprension.html#propuesta-de-solución",
    "href": "1_comprension.html#propuesta-de-solución",
    "title": "Comprensión del negocio",
    "section": "0.1. Propuesta de solución",
    "text": "0.1. Propuesta de solución\nProponemos desarrollar un sistema de Help Desk inteligente basado en Inteligencia Artificial Generativa que utilice la metodología de Recuperación Aumentada por Generación (RAG) y modelos de lenguaje grandes (LLM). Este sistema actuará como una herramienta de apoyo para los ingenieros, proporcionando respuestas inmediatas a sus consultas técnicas. El sistema permitirá una clasificación y recuperación de información más eficiente, y generará respuestas contextualizadas a partir de la base de conocimientos interna del CENACE."
  },
  {
    "objectID": "1_comprension.html#objetivos",
    "href": "1_comprension.html#objetivos",
    "title": "Comprensión del negocio",
    "section": "0.2. Objetivos",
    "text": "0.2. Objetivos\nEl objetivo principal es elaborar un sistema de Help Desk que utilice la base de conocimientos del CENACE y modelos de lenguaje grande para que los ingenieros tengan apoyo inmediato y puedan tomar decisiones rápidas al alcance de la mano. Con esto, se espera ahorrar tiempo en la clasificación, recuperación y generación de la información técnica, y a su vez, nutrir la base de conocimientos con las soluciones generadas."
  },
  {
    "objectID": "1_comprension.html#terminología",
    "href": "1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3. Terminología",
    "text": "0.3. Terminología\n\nRAG (Retrieval-Augmented Generation): Un enfoque de IA que combina la recuperación de información con la generación de lenguaje, para crear respuestas más precisas y contextualizadas.\nLLM (Large Language Model): Modelo de lenguaje grande capaz de comprender y generar texto similar al humano, como el modelo gemma3:4b que se utilizará en el proyecto.\nOllama: Un framework que permite ejecutar modelos de lenguaje grandes de código abierto de forma local.\nVector Embeddings: Representaciones numéricas de texto que capturan su significado semántico, facilitando la búsqueda de información similar.\nBase de datos vectorial: Una base de datos optimizada para almacenar y buscar vector embeddings.\nFastAPI: Un framework web de Python de alto rendimiento para construir APIs.\nMongoDB: Una base de datos NoSQL que se utilizará para almacenar el historial de conversaciones, tickets y la documentación original."
  },
  {
    "objectID": "1_comprension.html#beneficios",
    "href": "1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4. Beneficios",
    "text": "0.4. Beneficios\n\nInnovación en el soporte técnico: Introducir un nuevo enfoque para acceder a la información técnica, brindando una experiencia más personalizada y eficiente para los ingenieros.\nOptimización del flujo de trabajo: El sistema permitirá a los ingenieros ahorrar tiempo en la búsqueda de información, lo que se traducirá en una mayor eficiencia operativa y una toma de decisiones más rápida.\nPreservación del conocimiento: El sistema ayuda a estructurar y hacer accesible la vasta base de conocimientos del CENACE, garantizando que el conocimiento institucional no se pierda.\nClasificación y sugerencia automática: El sistema puede ayudar a clasificar los tickets de incidentes y sugerir soluciones, lo que agiliza el proceso de resolución."
  },
  {
    "objectID": "1_comprension.html#costos",
    "href": "1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5. Costos",
    "text": "0.5. Costos\n\nTiempo: El proyecto tiene un plazo estimado para desarrollar una versión funcional que pueda ser evaluada y mejorada.\nFinancieros: Aunque se utilizan modelos y herramientas de código abierto, se consideran costos asociados al hardware necesario para ejecutar los modelos de manera local (servidores, etc.)."
  },
  {
    "objectID": "3_preparacion.html",
    "href": "3_preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "La fase de preparación de datos es crucial para el funcionamiento del sistema RAG. Su objetivo es convertir la información no estructurada, proveniente de los documentos técnicos del CENACE, en un formato que permita una búsqueda eficiente y una recuperación semántica de alta calidad. Este proceso se divide en tres etapas principales: extracción, transformación y carga de los datos.\n\n\nLa principal fuente de información son los documentos técnicos en formato PDF, así como la base de datos de incidentes resueltos. Para la ingesta de documentos, el sistema utiliza el módulo doccollection.py, específicamente la clase DisjointCollection.\n\nIngesta de archivos: Los documentos PDF se cargan y procesan de forma individual utilizando la librería PyPDF2. Se extrae el texto de cada página, junto con sus metadatos inherentes (título, autor, etc.).\nFragmentación de texto (chunking): El texto extraído se divide en fragmentos lógicos o “chunks” para preservar el contexto de la información. El TextSplitter del módulo doccollection está configurado con los siguientes parámetros para optimizar la cohesión del texto:\n\nTamaño del chunk: 1500 caracteres.\nSolapamiento (overlap): 200 caracteres. Este solapamiento asegura que la información contextual clave no se pierda en los límites de cada fragmento.\n\nAsignación de metadatos: A cada chunk se ke asignan metadatos esenciales, como el nombre del archivo de origen (filename), el número de página (page_number), y un identificador único de referencia (reference). Estos metadatos son vitales para referenciar las fuentes en las respuestas del LLM, evitando alucinaciones y permitiendo al usuario validar la información."
  },
  {
    "objectID": "3_preparacion.html#extracción-de-los-datos",
    "href": "3_preparacion.html#extracción-de-los-datos",
    "title": "Preparación de los datos",
    "section": "",
    "text": "La principal fuente de información son los documentos técnicos en formato PDF, así como la base de datos de incidentes resueltos. Para la ingesta de documentos, el sistema utiliza el módulo doccollection.py, específicamente la clase DisjointCollection.\n\nIngesta de archivos: Los documentos PDF se cargan y procesan de forma individual utilizando la librería PyPDF2. Se extrae el texto de cada página, junto con sus metadatos inherentes (título, autor, etc.).\nFragmentación de texto (chunking): El texto extraído se divide en fragmentos lógicos o “chunks” para preservar el contexto de la información. El TextSplitter del módulo doccollection está configurado con los siguientes parámetros para optimizar la cohesión del texto:\n\nTamaño del chunk: 1500 caracteres.\nSolapamiento (overlap): 200 caracteres. Este solapamiento asegura que la información contextual clave no se pierda en los límites de cada fragmento.\n\nAsignación de metadatos: A cada chunk se ke asignan metadatos esenciales, como el nombre del archivo de origen (filename), el número de página (page_number), y un identificador único de referencia (reference). Estos metadatos son vitales para referenciar las fuentes en las respuestas del LLM, evitando alucinaciones y permitiendo al usuario validar la información."
  },
  {
    "objectID": "3_preparacion.html#transformación-de-los-datos",
    "href": "3_preparacion.html#transformación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "2. Transformación de los datos",
    "text": "2. Transformación de los datos\nUna vez que los documentos se han dividido en chunks, se transforman en una representación numérica que la computadora puede entender y procesar eficientemente.\n\nVectorización: Cada chunk de texto es procesado por el modelo de embeddings bge-m3:latest, implementado en la clase OllamaEmbedder.\nRepresentación vectorial: El modelo convierte el texto en un vector numérico de alta dimensión. Estos vectores capturan el significado semántico del texto; los chunks con un significado similar se agrupan en un espacio vectorial. Este enfoque va más allá de la simple búsqueda por palabres clave, ya que permite al sistema encontrar información relevante incluso si la consulta utiliza un vocabulario o una sintaxis diferente."
  },
  {
    "objectID": "3_preparacion.html#carga-de-los-datos",
    "href": "3_preparacion.html#carga-de-los-datos",
    "title": "Preparación de los datos",
    "section": "3. Carga de los datos",
    "text": "3. Carga de los datos\nLos vectores generados se almacenan en una base de datos vectorial optimizada para la búsqueda de similitud.\n\nBase de datos vectorial: Se utiliza FAISS (Facebook AI Similarity Search), una librería de código abierto para la búsqueda eficiente en grandes conjuntos de vectores. FAISS indexa los vectores de manera que las consultas de similitud se puedan ejecutar en milisegundos.\nPersistencia: La clase FAISSVectorStore es responsable de la carga y el almacenamiento de los vectores. El índice de FAISS se guarda en un archivo local (index.faiss), mientras que los metadatos de los chunks se almacenan en un diccionario (index.pkl).\nBúsqueda semántica: Una vez cargados los datos, la base de datos vectorial está lista para recibir consultas. Cuando un usuario envía una pregunta, esta se convierte en un vector, que luego se utiliza para encontrar los vectores más cercanos (los chunks más relevantes) en el índice de FAISS."
  },
  {
    "objectID": "3_preparacion.html#flujo-de-preparación",
    "href": "3_preparacion.html#flujo-de-preparación",
    "title": "Preparación de los datos",
    "section": "4. Flujo de preparación",
    "text": "4. Flujo de preparación\nEl proceso completo es orquestado por la clase RAG y es ejecutado como un flujo de trabajo de indexación:\n\nSolicitud de carga: El usuario sube un documento PDF a través del endpoint /documents.\nManejo de la ingesta: El rag.py recibe el archivo y delega su procesamiento al doccollection.\nGeneración de chunks y metadatos: doccollection lee el PDF, extrae el texto y lo divide en chunks. Asigna metadatos como el ID del documento, el nombre del archivo y el número de página a cada uno de ellos.\nVectorización y almacenamiento: Cada chunk y sus metadatos asociados se envían al vectorstore, que a su vez utiliza el embedder para obtener su representación vectorial. Finalmente, los vectores se agregan al índice de FAISS, asegurando que el conocimiento esté disponible para futuras consultas."
  },
  {
    "objectID": "5_despliegue.html",
    "href": "5_despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción.\nSe tuvieron pláticas con el equipo de Databricks para colaborar en el proyecto y nos ayuden en el despliegue del sistema en un ambiente acorde a las necesidades de poder computacional para que fluya debidamente."
  },
  {
    "objectID": "5_despliegue.html#revisión-del-proceso",
    "href": "5_despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción.\nSe tuvieron pláticas con el equipo de Databricks para colaborar en el proyecto y nos ayuden en el despliegue del sistema en un ambiente acorde a las necesidades de poder computacional para que fluya debidamente."
  },
  {
    "objectID": "5_despliegue.html#plan-de-implementación",
    "href": "5_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de implementación",
    "text": "2. Plan de implementación\nEl plan de despliegue se centra en migrar la arquitectura de desarrollo a un entorno de producción escalable, manteniendo la modularidad del sistema y optimizando el rendimiento. Se consideran las siguientes fases clave:\n\n2.1. Arquitectura de producción\nLa arquitectura final para el despliegue estará compuesta por los siguientes componentes clave, implementados en un entorno de producción como Databricks o una plataforma similar:\n\nServidor de la API (backend): Implementación de la API desarrollada en FastAPI (main.py, chat.py) en un servidor escalable. Este servidor manejará las peticiones de los usuarios, coordinará las operaciones del RAG y se comunicará con la base de datos y el LLM.\nModelo de lenguaje (LLM): El modelo de lenguaje gemma3:4b se desplegará en un servidor con aceleración por GPU para asegurar un rendimiento óptimo en la generación de respuestas.\nBase de datos vectorial: La base de datos vectorial de FAISS, que almacena los embeddings de los documentos (vectorstore.py), se mantendrá, pero se integrará con un sistema de almacenamiento persistente y escalable en la nube para garantizar la disponibilidad y el rendimiento.\nBase de datos de historial y tickets (MongoDB): La base de datos de MongoDB, utilizada para almacenar el historial de conversaciones y la gestión de tickets, se migrará a un servicio de bases de datos gestionado en la nube para asegurar la persistencia y la seguridad de los datos.\n\n\n\n2.2. Flujo de despliegue\nEl flujo propuesto para el despliegue es el siguiente:\n\nContenerización: Empaquetar la aplicación de FastAPI, el LLM y las dependencias en contenedores. Esto asegura que el entorno de ejecución sea consistente en todas las etapas del despliegue.\nOrquestación: Utilizar una herramienta de orquestación para gestionar los contenedores, facilitando la escalabilidad, el balanceo de carga y la recuperación automática en caso de fallos.\nMonitoreo y logging: Implementar un sistema de monitoreo que rastree el rendimiento del LLM, el uso de recursos y el flujo de la API. Esto permitirá identificar cuellos de botella y errores en tiempo real.\nIntegración con el sistema de Help Desk: El paso final es integrar la API del chatbot directamente en la plataforma de Help Desk de CENACE, permitiendo que los ingenieros puedan interactuar con el sistema desde su entorno de trabajo habitual."
  }
]