[
  {
    "objectID": "6_documentacion.html",
    "href": "6_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\n\n\n\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd proyectoCenace\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nLevantar el backend con Uvicorn:\n\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\n\n\n\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd proyectoCenace\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nLevantar el backend con Uvicorn:\n\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#documentación-técnica-del-código",
    "href": "6_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2. Documentación técnica del código",
    "text": "2. Documentación técnica del código\n\n2.1. Estructura de carpetas y módulos\n\n\n2.2. Modelos LLM utilizados\n\n\n2.3. Puntos de entrada y funciones clave"
  },
  {
    "objectID": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3. Guía de entrenamiento y mejora",
    "text": "3. Guía de entrenamiento y mejora\n\n3.1. Generación de la base de datos vectorial\n\n\n3.2. Flujo de la interacción"
  },
  {
    "objectID": "6_documentacion.html#diagrama-de-arquitectura",
    "href": "6_documentacion.html#diagrama-de-arquitectura",
    "title": "Documentación",
    "section": "4. Diagrama de arquitectura",
    "text": "4. Diagrama de arquitectura"
  },
  {
    "objectID": "4_modelado.html",
    "href": "4_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "4_modelado.html#modelado",
    "href": "4_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "4_modelado.html#evaluación",
    "href": "4_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa fase de evaluación es crucial para validar el desempeño del sistema y asegurar que cumple con los objetivos del proyecto. La evaluación se realiza a través de pruebas manuales y automatizadas.\n\n2.1 Criterios de evaluación"
  },
  {
    "objectID": "2_comprension_datos.html",
    "href": "2_comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "2_comprension_datos.html#recolección-de-los-datos",
    "href": "2_comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "2_comprension_datos.html#descripción-de-los-datos",
    "href": "2_comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2. Descripción de los datos",
    "text": "0.2. Descripción de los datos\nA partir de los datos extraídos, se obtuvo la siguiente información:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2817 entries, 0 to 2816\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   titulo       2817 non-null   object\n 1   descripcion  2817 non-null   object\n 2   solucion     1159 non-null   object\n 3   categories   2817 non-null   object\n 4   fecha        2817 non-null   object\ndtypes: object(5)\nmemory usage: 110.2+ KB\nLas variables se describen a continuación:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\ntitulo\nTítulo del incidente en cuestión.\nTexto\n\n\ndescripcion\nDesarrollo de la problemática y explicación del incidente.\nTexto\n\n\nsolucion\nExplicación de cómo se llegó a la solución.\nTexto\n\n\ncategories\nCategoría a la que pertenece la problemática.\nTexto\n\n\nfecha\nFecha.\nTexto"
  },
  {
    "objectID": "2_comprension_datos.html#exploración-de-los-datos",
    "href": "2_comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de categorías\nLa exploración comenzó con el análisis del número de categorías únicas en los tickets. Esto es crucial, ya que parte del proyecto es desarrollar un modelo capaz de clasificar o asignar una categoría a nuevos tickets a partir de su contenido.\ncategories\nADTR SP7 &gt; SCADA                                     1410\nINTRANET Y SOPORTE DE APLICACIONES                    379\nCOMPUTO Y PERIFERICOS                                 257\nSEGURIDAD INFORMATICA                                 166\nCORREO ELECTRONICO                                     97\nADTR SP7 &gt; SIREL                                       89\nADTR SP7                                               82\nTELEFONIA Y HERRAMIENTAS COLABORATIVAS                 55\nINFRAESTRUCTURA BASICA Y DE SERVICIOS PROPIOS          51\nADTR &gt; Consulta                                        38\nADTR SP7 &gt; Historico                                   37\nADTR SP7 &gt; SIGUARD                                     32\nINTERNET                                               32\nADTR SP7 &gt; Hospedaje                                   23\nADOMEM                                                 16\nADTR                                                   11\nDESARROLLO DE APLICACIONES                             10\nOPERACION DE RED DE DATOS                               9\nMESA DE SERVICIO                                        8\nBASE DE DATOS                                           7\nADTR &gt; Hospedaje de Aplicativos de Potencia (EMS)       6\nMONITOREO DE ACTIVOS DE TIC                             2\nName: count, dtype: int64\nAl observar la distribución, se decidió trabajar únicamente con categorías que tuvieran más de 20 incidentes, considerando la cantidad de datos necesaria para los procesos de entrenamiento, validación y prueba. Esta selección resultó en un total de 14 categorías.\n\n\n0.3.2. Distribución de las palabras\nEn esta sección se analiza la cantidad de palabras utilizadas en los títulos, descripciones y soluciones. Esto nos permite entender la cantidad de información disponible que puede ser útil para resolver problemas recurrentes y para el desarrollo del modelo de clasificación.\n\n\n\nDistribución de palabras en los títulos\n\n\nA partir de esta gráfica podemos notar que alrededor de 9 palabras promedio son las que se utilizan en los títulos; lo cual es normal dado que tendemos a englobar las problemáticas en pocas palabras. Sin embargo, vemos que hay problemáticas que pueden pasar el promedio, hasta llegar a las 30 palabras aproximadamente.\n\n\n\nDistribución de palabras en las descripciones\n\n\nPara las descripciones vemos que el promedio es mayor, aproximadamente hasta las 88 palabras, aunque en la mayoría de los casos son menos las que se utilizan. Esto también es normal ya que aquí es donde las personas desarrollan los detalles del incidente el cual están enfrentando. También vemos que hay incidentes que pueden tomar tantas palabras hasta llegar a las 400, 500, y hasta las 1000, aunque este último sea poco común.\n\n\n\nDistribución de palabras en las soluciones\n\n\nEn este caso, nosotros esperábamos que en esta sección hubiéramos encontrado una mayor cantidad de palabras, porque en este caso encontramos que en promedio se utilizan 23 palabras, donde frecuentemente son menos. También encontramos que no todos los incidentes contienen una descripción detallada de la solución o de los pasos que se siguieron para resolver la situación; vimos que solo el 41% de los incidentes contienen una explicación de la solución.\n\n\n0.3.3. Análisis de los bigramas más comunes\nSe realizó un análisis de los pares de palabras más frecuentes en los títulos y descripciones para identificar patrones en la forma en que se plantean las problemáticas.\n\n\n\nBigramas de los títulos\n\n\nEn los títulos lo primero que llama la atención es la codificación de ciertas palabras, ya que por detalles confidenciales, se codificaron nombres propios, sistemas, subestaciones, etc. Luego, podemos ver que se mencionan muchas veces los despliegues de distintos sistemas de software y actualizaciones.\n\n\n\nBigramas de las descripciones\n\n\nEn las descripciones vemos que en la gran mayoría de los casos son saludos hacia la persona que se están dirigiendo. Dejando de lado estos saludos, vemos que se mencionan detalles muy técnicos y específicos que solo expertos en el tema podrán entender."
  },
  {
    "objectID": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nA partir de los resultados previos, se determinó que la columna con menos información es la de soluciones. Solo el 41% de los registros contienen una explicación, con un promedio de 23 palabras, aunque en muchos casos la cantidad es menor.\nAl discutir esta situación con los expertos, se descubrió que el desarrollo y la documentación de soluciones no es una práctica común en el departamento, lo que resulta en una pérdida de conocimiento. Esto motivó la búsqueda de una propuesta para mejorar la persistencia de las soluciones."
  },
  {
    "objectID": "0_home.html",
    "href": "0_home.html",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "0_home.html#introducción",
    "href": "0_home.html#introducción",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "1_comprension.html",
    "href": "1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "El Centro Nacional de Control de Energía (CENACE) es el organismo encargado de la planeación y el control operativo del Sistema Eléctrico Nacional (SEN). El CENACE cuenta con una mesa de ayuda (Help Desk), la cual provee soporte y le da seguimiento a incidentes reportados en todo el país. En este trabajo nos restringimos a los incidentes reportados para la zona noroeste, la cual se conforma de los estados de Sonora y Sinaloa. Los incidentes se reportan en formato de tickets y son gestionados por los ingenieros de la organización.\nTeniendo en cuenta la naturaleza crítica del sector energético, la constante evolución tecnológica y la gran cantidad de documentación técnica existente, surge la necesidad de optimizar el acceso a la información. La búsqueda manual de esta información para resolver incidentes puede ser un proceso lento, impactando la eficiencia operativa. Además de aprovechar la base de conocimientos que ya se tiene para proponer soluciones a problemas previamente vistos."
  },
  {
    "objectID": "1_comprension.html#propuesta-de-solución",
    "href": "1_comprension.html#propuesta-de-solución",
    "title": "Comprensión del negocio",
    "section": "0.1. Propuesta de solución",
    "text": "0.1. Propuesta de solución\nProponemos desarrollar un sistema de Help Desk inteligente basado en Inteligencia Artificial Generativa que utilice la metodología de Recuperación Aumentada por Generación (RAG) y modelos de lenguaje grandes (LLM). Este sistema actuará como una herramienta de apoyo para los ingenieros, proporcionando respuestas inmediatas a sus consultas técnicas. El sistema permitirá una clasificación y recuperación de información más eficiente, y generará respuestas contextualizadas a partir de la base de conocimientos interna del CENACE."
  },
  {
    "objectID": "1_comprension.html#objetivos",
    "href": "1_comprension.html#objetivos",
    "title": "Comprensión del negocio",
    "section": "0.2. Objetivos",
    "text": "0.2. Objetivos\nEl objetivo principal es elaborar un sistema de Help Desk que utilice la base de conocimientos del CENACE y modelos de lenguaje grande para que los ingenieros tengan apoyo inmediato y puedan tomar decisiones rápidas al alcance de la mano. Con esto, se espera ahorrar tiempo en la clasificación, recuperación y generación de la información técnica, y a su vez, nutrir la base de conocimientos con las soluciones generadas."
  },
  {
    "objectID": "1_comprension.html#terminología",
    "href": "1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3. Terminología",
    "text": "0.3. Terminología\n\nRAG (Retrieval-Augmented Generation): Un enfoque de IA que combina la recuperación de información con la generación de lenguaje, para crear respuestas más precisas y contextualizadas.\nLLM (Large Language Model): Modelo de lenguaje grande capaz de comprender y generar texto similar al humano, como el modelo gemma3:4b que se utilizará en el proyecto.\nOllama: Un framework que permite ejecutar modelos de lenguaje grandes de código abierto de forma local.\nVector Embeddings: Representaciones numéricas de texto que capturan su significado semántico, facilitando la búsqueda de información similar.\nBase de datos vectorial: Una base de datos optimizada para almacenar y buscar vector embeddings.\nFastAPI: Un framework web de Python de alto rendimiento para construir APIs.\nMongoDB: Una base de datos NoSQL que se utilizará para almacenar el historial de conversaciones, tickets y la documentación original."
  },
  {
    "objectID": "1_comprension.html#beneficios",
    "href": "1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4. Beneficios",
    "text": "0.4. Beneficios\n\nInnovación en el soporte técnico: Introducir un nuevo enfoque para acceder a la información técnica, brindando una experiencia más personalizada y eficiente para los ingenieros.\nOptimización del flujo de trabajo: El sistema permitirá a los ingenieros ahorrar tiempo en la búsqueda de información, lo que se traducirá en una mayor eficiencia operativa y una toma de decisiones más rápida.\nPreservación del conocimiento: El sistema ayuda a estructurar y hacer accesible la vasta base de conocimientos del CENACE, garantizando que el conocimiento institucional no se pierda.\nClasificación y sugerencia automática: El sistema puede ayudar a clasificar los tickets de incidentes y sugerir soluciones, lo que agiliza el proceso de resolución."
  },
  {
    "objectID": "1_comprension.html#costos",
    "href": "1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5. Costos",
    "text": "0.5. Costos\n\nTiempo: El proyecto tiene un plazo estimado para desarrollar una versión funcional que pueda ser evaluada y mejorada.\nFinancieros: Aunque se utilizan modelos y herramientas de código abierto, se consideran costos asociados al hardware necesario para ejecutar los modelos de manera local (servidores, etc.)."
  },
  {
    "objectID": "3_preparacion.html",
    "href": "3_preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Los datos principales vienen de la información en formatos PDF. Estos archivos se localizan en una carpeta y se procesan uno por uno, separando cada contenido dentro de ellas en fragmentos de texto o “chunks”.\n\n\n\nCada fragmento de texto se procesa a través de un modelo de embeddings y se les asigna una representación vectorial, esto para cada trozo de cada PDF para todos los PDF’s.\n\n\n\nTodos estos embeddings, o representaciones vectoriales, se almacenan en una base de datos vectorial de FAISS. Un archivo local que se carga y contiene vectores indexados con los cuales se pueden hacer las búsquedas semánticas de las consultas que hagan los usuarios."
  },
  {
    "objectID": "3_preparacion.html#extracción-de-los-datos",
    "href": "3_preparacion.html#extracción-de-los-datos",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Los datos principales vienen de la información en formatos PDF. Estos archivos se localizan en una carpeta y se procesan uno por uno, separando cada contenido dentro de ellas en fragmentos de texto o “chunks”."
  },
  {
    "objectID": "3_preparacion.html#transformación-de-los-datos",
    "href": "3_preparacion.html#transformación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Cada fragmento de texto se procesa a través de un modelo de embeddings y se les asigna una representación vectorial, esto para cada trozo de cada PDF para todos los PDF’s."
  },
  {
    "objectID": "3_preparacion.html#carga-de-los-datos",
    "href": "3_preparacion.html#carga-de-los-datos",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Todos estos embeddings, o representaciones vectoriales, se almacenan en una base de datos vectorial de FAISS. Un archivo local que se carga y contiene vectores indexados con los cuales se pueden hacer las búsquedas semánticas de las consultas que hagan los usuarios."
  },
  {
    "objectID": "5_despliegue.html",
    "href": "5_despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción.\nSe tuvieron pláticas con el equipo de Databricks para colaborar en el proyecto y nos ayuden en el despliegue del sistema en un ambiente acorde a las necesidades de poder computacional para que fluya debidamente."
  },
  {
    "objectID": "5_despliegue.html#revisión-del-proceso",
    "href": "5_despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción.\nSe tuvieron pláticas con el equipo de Databricks para colaborar en el proyecto y nos ayuden en el despliegue del sistema en un ambiente acorde a las necesidades de poder computacional para que fluya debidamente."
  },
  {
    "objectID": "5_despliegue.html#plan-de-implementación",
    "href": "5_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de implementación",
    "text": "2. Plan de implementación\nEl plan de despliegue se centra en migrar la arquitectura de desarrollo a un entorno de producción escalable, manteniendo la modularidad del sistema y optimizando el rendimiento. Se consideran las siguientes fases clave:\n\n2.1. Arquitectura de producción\nLa arquitectura final para el despliegue estará compuesta por los siguientes componentes clave, implementados en un entorno de producción como Databricks o una plataforma similar:\n\nServidor de la API (backend): Implementación de la API desarrollada en FastAPI (main.py, chat.py) en un servidor escalable. Este servidor manejará las peticiones de los usuarios, coordinará las operaciones del RAG y se comunicará con la base de datos y el LLM.\nModelo de lenguaje (LLM): El modelo de lenguaje gemma3:4b se desplegará en un servidor con aceleración por GPU para asegurar un rendimiento óptimo en la generación de respuestas.\nBase de datos vectorial: La base de datos vectorial de FAISS, que almacena los embeddings de los documentos (vectorstore.py), se mantendrá, pero se integrará con un sistema de almacenamiento persistente y escalable en la nube para garantizar la disponibilidad y el rendimiento.\nBase de datos de historial y tickets (MongoDB): La base de datos de MongoDB, utilizada para almacenar el historial de conversaciones y la gestión de tickets, se migrará a un servicio de bases de datos gestionado en la nube para asegurar la persistencia y la seguridad de los datos.\n\n\n\n2.2. Flujo de despliegue\nEl flujo propuesto para el despliegue es el siguiente:\n\nContenerización: Empaquetar la aplicación de FastAPI, el LLM y las dependencias en contenedores. Esto asegura que el entorno de ejecución sea consistente en todas las etapas del despliegue.\nOrquestación: Utilizar una herramienta de orquestación para gestionar los contenedores, facilitando la escalabilidad, el balanceo de carga y la recuperación automática en caso de fallos.\nMonitoreo y logging: Implementar un sistema de monitoreo que rastree el rendimiento del LLM, el uso de recursos y el flujo de la API. Esto permitirá identificar cuellos de botella y errores en tiempo real.\nIntegración con el sistema de Help Desk: El paso final es integrar la API del chatbot directamente en la plataforma de Help Desk de CENACE, permitiendo que los ingenieros puedan interactuar con el sistema desde su entorno de trabajo habitual."
  }
]