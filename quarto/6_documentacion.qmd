---
title: "Documentación"
format: 
  html:
    page-layout: article
toc-title: "Tabla de Contenidos"
toc: true
toc-depth: 3
---

::: {style="text-align: justify"}
## 1. Manual de instalación y despliegue

### 1.1. Configuraciones importantes

* El proyecto está diseñado para ser desplegado en entornos **Linux** o **Windows** con Python `3.12.9`. Requiere acceso a **Ollama** (para la ejecución de modelos open-source), así como conectividad a una instancia de **MongoDB** para el registro del historial de conversaciones y tickets.
* La aplicación backend se expone a través de **FastAPI** en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.
* Todas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (`.env`), garantizando la seguridad y facilidad de configuración.

### 1.2. Requisitos del sistema

* **Python**: Versión 3.12.9
* **Pip**: Última versión
* **UV**: Última versión (gestor de paquetes y entornos)
* **Ollama**: Instalado y en ejecución en el servidor para el hosting de modelos open-source.
* **MongoDB**: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.

### 1.3. Dependencias principales del sistema

### 1.4. Instalación del sistema

1. Clonar el repositorio:
```bash
git clone https://github.com/anmerino-pnd/proyectoCenace
cd proyectoCenace
```

2. Configurar el entorno:
```bash
pip install uv
uv venv
source .venv\Scripts\activate
# o `.venv\Scripts\activate` para Windows
uv pip install -e .

```

3. Configurar Ollama:

   Verifica que el servicio de Ollama esté instalado y activo, y que el modelo `gemma3:12b` y `bge-m3:latest` estén disponible.

```bash
curl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama
ollama serve
ollama list # Para verificar que el modelo gemma3:12b esté descargado y listo
ollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca
ollama pull bge-m3:latest
```

4. Configurar variables de entorno:

   Antes de levantar el *backend*, asegurarse de que el archivo `.env` en la raíz del proyecto contenga las siguientes variables con sus valores correctos. 

```python
# Conexión a la base de datos SQL
ip=
port=
user=
pwd=
db=

# Clave de la API de OpenAI para correr sus modelos
OPENAI_API_KEY=

# Configuración para el servicio de fichas técnicas
url= '' 
Token-api=''
Token-ct=''
Content-Type=''
Cookie=''

dominio=""
boundary=''

# Conexión a MongoDB
MONGO_URI = "mongodb://" # En la URI debe estar incrustrado el nombre de la DB
MONGO_COLLECTION_SESSIONS = ""
MONGO_COLLECTION_MESSAGE_BACKUP = ""
MONGO_COLLECTION_PRODUCTS = ""
MONGO_COLLECTION_SALES = ""         
MONGO_COLLECTION_SPECIFICATIONS = ""
```

5. Levantar el backend con Uvicorn:

  Este comando inicia la API, especificando el número del puerto

```bash
nohup uvicorn main:app --reload &
```
  El uso de `nogup` y `&` asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.

7. Verificar logs:

  Al correr la API con `nohup`, este genera un archivo `nohup.out`, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:

```bash
tail -f nohup.out
```
:::

::: {style="text-align: justify"}
## 2. Documentación técnica del código

### 2.1. Estructura de carpetas y módulos



### 2.2. Modelos LLM utilizados


### 2.3. Puntos de entrada y funciones clave


:::

::: {style="text-align: justify"}
## 3. Guía de entrenamiento y mejora

### 3.1.  Generación de la base de datos vectorial



### 3.2. Flujo de la interacción
:::


::: {style="text-align: justify"}
## 4. Diagrama de arquitectura


:::