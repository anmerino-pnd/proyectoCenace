---
title: "Documentación"
format: 
  html:
    page-layout: article
toc-title: "Tabla de Contenidos"
toc: true
toc-depth: 3
---

::: {style="text-align: justify"}
## 1. Manual de instalación y despliegue

### 1.1. Configuraciones importantes

* El proyecto está diseñado para ser desplegado en entornos **Linux** o **Windows** con Python `3.12.9`. Requiere acceso a **Ollama** (para la ejecución de modelos open-source), así como conectividad a una instancia de **MongoDB** para el registro del historial de conversaciones y tickets.
* La aplicación backend se expone a través de **FastAPI** en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.
* Todas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (`.env`), garantizando la seguridad y facilidad de configuración.

### 1.2. Requisitos del sistema

* **Python**: Versión 3.12.9
* **Pip**: Última versión
* **UV**: Última versión (gestor de paquetes y entornos)
* **Ollama**: Instalado y en ejecución en el servidor para el hosting de modelos open-source.
* **MongoDB**: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.

### 1.3. Dependencias principales del sistema

### 1.4. Instalación del sistema

1. Clonar el repositorio:
```bash
git clone https://github.com/anmerino-pnd/proyectoCenace
cd cenacellm
```

2. Configurar el entorno:
```bash
pip install uv
uv venv
source .venv\Scripts\activate
# o `.venv\Scripts\activate` para Windows
uv pip install -e .
```

3. Configurar Ollama:

   Verifica que el servicio de Ollama esté instalado y activo, y que el modelo `gemma3:12b` y `bge-m3:latest` estén disponible.

```bash
curl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama
ollama serve
ollama list # Para verificar que el modelo gemma3:12b esté descargado y listo
ollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca
ollama pull bge-m3:latest
```

4. Configurar variables de entorno:

   Antes de levantar el *backend*, asegurarse de que el archivo `.env` en la raíz del proyecto contenga las siguientes variables con sus valores correctos. 

```python
# Servidor donde está corriendo Ollama
OLLAMA_BASE_URL=""

# Conexión a MongoDB
MONGO_URI = "mongodb://localhost:27017" 
DB_NAME = "CENACE_LLM"
```

5. Levantar el backend con Uvicorn:

  Este comando inicia la API, especificando el número del puerto

```bash
nohup uvicorn cenacellm.API.main:app --reload &
```
  El uso de `nogup` y `&` asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.

7. Verificar logs:

  Al correr la API con `nohup`, este genera un archivo `nohup.out`, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:

```bash
tail -f nohup.out
```
:::

::: {style="text-align: justify"}
## 2. Documentación técnica del código
La solución se basa en una arquitectura de **Recuperación Aumentada con Generación (RAG)**. La estructura modular del código, organizada en paquetes de Python, permite una clara separación de responsabilidades.

### 2.1. Estructura de carpetas y módulos

* `API/`
  * `chat.py`: Contiene los *endpoints* de FastAPI para interactuar con el chatbot.
  * `main.py`: Archivo principal que define la aplicación FastAPI y monta los *endpoints*.

* `ollama/`
  * `assistant.py`: Clase que encapsula la lógica para generar embeddings utilizando el modelo `bge-m3:latest` de Ollama.

* `doccollection.py`: Módulo que maneja la carga y el procesamiento de documentos (PDF's) para generar fragmentos de texto.

* `rag.py`: Clase principal del sistema RAG que integra el `assistant`, el `doccollection` y el `vectorstore`.

* `vectorstore.py`: Módulo que implementa la base de datos vectorial con FAISS.

* `settings/`
  * `clients.py`: Archivo de configuración que establece la conexión con la base de datos y el cliente de Ollama.
  * `config.py`: Define las rutas de directorios para los vectores y los documentos procesados.

* `tools/`
  * `assistant.py`: Clase base abstracta para el asistente LLM.
  * `doccollection.py`: Clase base abstracta para la colección de documentos.
  * `embedder.py`: Clase base abstracta para el generador de embeddings.
  * `vectorstore.py`: Clase base abstracta para almacén de vectores.

* `types.py`: Módulo que define modelos de datos con Pydantic para tipado de datos como `Text`, `TextMetadata`, `Question`, etc.

### 2.2. Modelos LLM utilizados
El flujo de información en el sistema RAG sigue dos rutas principales:

1.  Indexación de documentos:
  * Los archivos PDF son cargados y procesados por el módulo `doccollection.py`.
  * `doccollection` divide cada documento en fragmentos.
  * Cada fragmento es enviado al `embedder.py` para generar su representación vectorial.
  * Los vectores resultantes se almacenan en la base de datos vectorial de FAISS, implementada en `vectorstore.py`, junto con sus metadatos.

2. Proceso de consulta (QA):
  * Una consulta de usuario llega el *endpoint* de `chat.py`.
  * La consulta es vectorizada por el `embedder`.
  * El `vectorstore` realiza una búsqueda de similitud semántica para recuperar los fragmentos de documento más relevantes.
  * Estos fragmentos se envían al `assistant.py`, que los utiliza como contexto.
  * El `assistant` utiliza el LLM (`gemma3:4b`) para generar una respuesta coherente y contextualizada.
  * La respuesta es devuelta al usuario a través del `chat.py` y el `main.py`.

### 2.3. Puntos de entrada y funciones clave

* **Gestión de conversaciones:** El módulo `assistant.py` gestiona el historial de conversación en MongoDB, permitiendo que el chatbot mantenga un contexto limitado con el usuario.
* **Gestión de tickets:** Las funciones `add_ticket` y `update_ticket_metadata` en `rag.py` y sus respectivos *endpoints* en `chat.py` demuestran la capacidad del sistema para interactuar y actualizar una base de datos de tickets.
* **Bucle de retroalimentación:** La funcionalidad `has_liked_solution_in_conversation` permite identificar y potencialmente re-indexar soluciones validadas por los usuarios, mejorando continuamente la base de conocimientos.

:::

::: {style="text-align: justify"}
## 3. Guía de entrenamiento y mejora


### 3.1.  Generación de la base de datos vectorial



### 3.2. Flujo de la interacción
:::


::: {style="text-align: justify"}
## 4. Diagrama de arquitectura
El siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial.

![Arquitectura del sistema](./datos/image.png){width=100%}

### 4.1. Componentes clave de conversación y chat

* `POST /chat/stream`: Endpoint principal para la interacción conversacional. Recibe una consulta y un `conversation_id`, y devuelve una respuesta generada por el LLM en tiempo real a través de un stream.

* `GET /chat/history/{user_id}/{conversation_id}`: Recupera el historial de mensajes de una conversación específica.

* `POST /conversations`: Crea una nueva conversación, generando un `conversation_id` único.

* `GET /conversations/{user_id}`: Lista todas las conversaciones de un usuario, incluyendo sus títulos y la fecha de la última actualización.

* `DELETE /conversations`: Elimina una conversación específica y su historial de la base de datos.

* `PATCH /message-metadata`: Permite actualizar los metadatos de un mensaje, utilizado para la funcionalidad de "gustar" una solución.

### 4.2. Componentes clave de documentos y soluciones

* `POST /documents`: Permite cargar nuevos archivos (en formato PDF) a la base de datos vectorial para expandir la base de conocimientos.

* `GET /documents`: Lista todos los documentos que han sido procesados y están disponibles para la consulta.

* `DELETE /documents`: Elimina un documento específico de la base de datos vectorial, eliminando también su referencia y los fragmentos asociados.

* `POST /solutions`: Procesa y re-indexa soluciones "gustadas" por los usuarios, agregándolas como nuevos documentos a la base de datos vectorial para mejorar la precisión del sistema.

* `DELETE /solutions`: Elimina una solución específica de la base de datos vectorial.

### 4.3. Componentes clave de tickets

* `GET /tickets`: Recupera una lista de todos los tickets almacenados en la base de datos de MongoDB.

* `POST /tickets`: Permite añadir un nuevo ticket a la base de datos, con campos como título, descripción y categoría.

* `PATCH /tickets/{ticket_reference}`: Actualiza los metadatos de un ticket existente, como su estado de solución (`is_solved`).

:::