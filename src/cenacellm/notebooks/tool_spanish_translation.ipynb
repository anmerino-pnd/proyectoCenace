{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb99fc7-25b9-423c-8c25-21b79440d119",
   "metadata": {},
   "source": [
    "# Using implementation from 2025-02-23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb7454-ab65-41de-a24c-5b1a58070d0e",
   "metadata": {},
   "source": [
    "The previous experimentation gave us enough experience on how to use the provider's SDKs to implement the translator tool into the `elelem` library.\n",
    "\n",
    "Here's a brief summary of how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd769c23-8af0-4906-b878-dd7ec5e9294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = (\n",
    "    \"El rey de parangaricutirimicuaro se quiere \" \\\n",
    "    \"desparangaricutirimicuar, quien logre \" \\\n",
    "    \"desparangaricutirimicuarlo un gran \" \\\n",
    "    \"desparangaricutirimicuador será\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e5da234-18ad-403b-8e23-41113eec55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elelem.google.translator import GoogleTranslator\n",
    "from elelem.anthropic.translator import AnthropicTranslator\n",
    "from elelem.openai.translator import OpenAITranslator\n",
    "from elelem.mistral.translator import MistralTranslator\n",
    "from elelem.ollama.translator import OllamaTranslator\n",
    "from elelem.types import ElelemError, ElelemAPIResponseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff0b753d-56a5-45f5-be3c-81a2d2a703b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "googleT = GoogleTranslator()\n",
    "anthropicT = AnthropicTranslator()\n",
    "openaiT = OpenAITranslator()\n",
    "mistralT = MistralTranslator()\n",
    "ollamaT = OllamaTranslator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eae911d3-ef41-47d8-a2e6-171872e3486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE DETECTING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"google\",\n",
      "  \"model\": \"gemini-2.0-flash-lite-preview-02-05\",\n",
      "  \"operation\": \"models/generate_content\",\n",
      "  \"duration\": 0.8520453480005017,\n",
      "  \"input_tokens\": 128,\n",
      "  \"output_tokens\": 13,\n",
      "  \"timestamp\": \"2025-03-01T00:51:57.721688Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"GOOGLE DETECTING LANGUAGE...\")\n",
    "res, metadata = googleT.detect(txt)\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c22877e6-f80e-429a-91a5-23a6b152d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE TRANSLATING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\",\n",
      "  \"tgt_lang\": \"English\",\n",
      "  \"tgt_text\": \"The king of Parangaricutirimicuaro wants to unparangaricutirimicuar himself; whoever manages to unparangaricutirimicuar him will be a great unparangaricutirimicuator.\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"google\",\n",
      "  \"model\": \"gemini-2.0-flash-lite-preview-02-05\",\n",
      "  \"operation\": \"models/generate_content\",\n",
      "  \"duration\": 1.12276910699984,\n",
      "  \"input_tokens\": 154,\n",
      "  \"output_tokens\": 70,\n",
      "  \"timestamp\": \"2025-03-01T00:53:38.765087Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"GOOGLE TRANSLATING LANGUAGE...\")\n",
    "res, metadata = googleT.translate(txt, \"English\")\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b29a9d13-6bfc-4244-bb20-c74a357fbc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTHROPIC DETECTING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"es\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"anthropic\",\n",
      "  \"model\": \"claude-3-5-haiku-latest\",\n",
      "  \"operation\": \"messages/create\",\n",
      "  \"duration\": 1.5549830390000352,\n",
      "  \"input_tokens\": 749,\n",
      "  \"output_tokens\": 35,\n",
      "  \"timestamp\": \"2025-03-01T00:54:05.559974Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"ANTHROPIC DETECTING LANGUAGE...\")\n",
    "res, metadata = anthropicT.detect(txt)\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e76e785-bd6e-4fa3-8834-63341f7a7db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTHROPIC TRANSLATING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"es\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\",\n",
      "  \"tgt_lang\": \"English\",\n",
      "  \"tgt_text\": \"The king of Parangaricutirimicuaro wants to un-Parangaricutirimicuate himself, whoever manages to un-Parangaricutirimicuate him will be a great un-Parangaricutirimicuator\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"anthropic\",\n",
      "  \"model\": \"claude-3-5-haiku-latest\",\n",
      "  \"operation\": \"messages/create\",\n",
      "  \"duration\": 2.5343684600002234,\n",
      "  \"input_tokens\": 760,\n",
      "  \"output_tokens\": 115,\n",
      "  \"timestamp\": \"2025-03-01T00:54:18.013313Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"ANTHROPIC TRANSLATING LANGUAGE...\")\n",
    "res, metadata = anthropicT.translate(txt, \"English\")\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cd040b5-6890-4554-a197-4d45f10428a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI DETECTING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"openai\",\n",
      "  \"model\": \"gpt-4o-mini\",\n",
      "  \"operation\": \"beta/chat/completions/parse\",\n",
      "  \"duration\": 0.9883059069998126,\n",
      "  \"input_tokens\": 173,\n",
      "  \"output_tokens\": 7,\n",
      "  \"timestamp\": \"2025-03-01T00:54:26.430353Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"OPENAI DETECTING LANGUAGE...\")\n",
    "res, metadata = openaiT.detect(txt)\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b247c20-8cd6-4219-8c0c-94dce7b6a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI TRANSLATING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\",\n",
      "  \"tgt_lang\": \"English\",\n",
      "  \"tgt_text\": \"The king of Parangaricutirimícuaro wants to un-parangaricutirimícuaro himself; whoever manages to un-parangaricutirimícuaro him will be a great un-parangaricutirimícuador.\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"openai\",\n",
      "  \"model\": \"gpt-4o-mini\",\n",
      "  \"operation\": \"beta/chat/completions/parse\",\n",
      "  \"duration\": 3.1394193249998352,\n",
      "  \"input_tokens\": 220,\n",
      "  \"output_tokens\": 59,\n",
      "  \"timestamp\": \"2025-03-01T00:54:29.573563Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"OPENAI TRANSLATING LANGUAGE...\")\n",
    "res, metadata = openaiT.translate(txt, \"English\")\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0cd2426e-1d23-41b7-9825-c8f2a91a51c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTRAL DETECTING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"mistral\",\n",
      "  \"model\": \"mistral-small-latest\",\n",
      "  \"operation\": \"chat/parse\",\n",
      "  \"duration\": 1.2410207759994591,\n",
      "  \"input_tokens\": 120,\n",
      "  \"output_tokens\": 7,\n",
      "  \"timestamp\": \"2025-03-01T00:54:43.530921Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"MISTRAL DETECTING LANGUAGE...\")\n",
    "res, metadata = mistralT.detect(txt)\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d516a739-4585-4cd6-938b-490d3019d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTRAL TRANSLATING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\",\n",
      "  \"tgt_lang\": \"English\",\n",
      "  \"tgt_text\": \"The king of Parangaricutirimicuaro wants to be unparangaricutirimicuaro, whoever unparangaricutirimicuaro him will be a great unparangaricutirimicuador.\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"mistral\",\n",
      "  \"model\": \"mistral-small-latest\",\n",
      "  \"operation\": \"chat/parse\",\n",
      "  \"duration\": 1.605148411999835,\n",
      "  \"input_tokens\": 127,\n",
      "  \"output_tokens\": 62,\n",
      "  \"timestamp\": \"2025-03-01T00:54:45.139684Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"MISTRAL TRANSLATING LANGUAGE...\")\n",
    "res, metadata = mistralT.translate(txt, \"English\")\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd1d944e-0482-489f-b490-251f329a2e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA DETECTING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"ollama\",\n",
      "  \"model\": \"gemma2:9b\",\n",
      "  \"operation\": \"generate\",\n",
      "  \"duration\": 4.65059200900032,\n",
      "  \"input_tokens\": 126,\n",
      "  \"output_tokens\": 14,\n",
      "  \"timestamp\": \"2025-03-01T00:55:29.548534Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"OLLAMA DETECTING LANGUAGE...\")\n",
    "    res, metadata = ollamaT.detect(txt)\n",
    "    print(\"RESPONSE:\")\n",
    "    print(res.model_dump_json(indent=2))\n",
    "    print(\"\\nMETADATA:\")\n",
    "    print(metadata.model_dump_json(indent=2))\n",
    "except ElelemAPIResponseError as e:\n",
    "    print(\"message:\", e)\n",
    "    print(\"exception:\", e.exception)\n",
    "    print(\"Response:\", e.response)\n",
    "except ElelemError as e:\n",
    "    print(\"message:\", e)\n",
    "    print(\"exception:\", e.exception)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5138ef76-81a9-4a4e-be2b-9597952bf371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA TRANSLATING LANGUAGE...\n",
      "RESPONSE:\n",
      "{\n",
      "  \"src_lang\": \"Spanish\",\n",
      "  \"src_text\": \"El rey de parangaricutirimicuaro se quiere desparangaricutirimicuar, quien logre desparangaricutirimicuarlo un gran desparangaricutirimicuador será\",\n",
      "  \"tgt_lang\": \"English\",\n",
      "  \"tgt_text\": \"The king of Parangaricutirimícuaro wants to be deposed. Whoever deposes him will be a great deposer.\"\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "{\n",
      "  \"provider\": \"ollama\",\n",
      "  \"model\": \"gemma2:9b\",\n",
      "  \"operation\": \"generate\",\n",
      "  \"duration\": 5.246626543000275,\n",
      "  \"input_tokens\": 134,\n",
      "  \"output_tokens\": 46,\n",
      "  \"timestamp\": \"2025-03-01T00:55:34.798656Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"OLLAMA TRANSLATING LANGUAGE...\")\n",
    "res, metadata = ollamaT.translate(txt, \"English\")\n",
    "print(\"RESPONSE:\")\n",
    "print(res.model_dump_json(indent=2))\n",
    "print(\"\\nMETADATA:\")\n",
    "print(metadata.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a545f25-c970-44c1-99db-8b4737f3adeb",
   "metadata": {},
   "source": [
    "# Experimentation 2025-02-22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d04070-8ac7-4db9-9716-e8281a513bb0",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a3bc7c-5f09-4404-8c9c-4de0423cd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import elelem.clients as clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff02a2f-85d6-4434-bddb-81369dfeb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c205c6-5524-4be5-9809-08a6c9a0e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Literal, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59bc6f30-2ebd-4a70-ab8a-41a2b15f11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c33d9c9-d281-401b-b16b-1806af22afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ac995d-d6b5-4f73-b234-762a4a78cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21775883-8698-47fb-ae4e-492c43d0a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256691f-8a2f-4eb7-bdbd-d8324dd0bb9c",
   "metadata": {},
   "source": [
    "## Basic types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aabfc0d-9571-4605-a3ff-bc4be16ac31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestMetadata(BaseModel):\n",
    "    provider : Literal[\"google\", \"anthropic\", \"openai\", \"mistral\", \"ollama\"]\n",
    "    model : str\n",
    "    duration : float\n",
    "    input_tokens : int\n",
    "    output_tokens : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50383d8-0912-48e4-ae54-d6d5a43b33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDetectLanguageResult(BaseModel):\n",
    "    src_lang : str = Field(description=\"Language of the source text\")\n",
    "    \n",
    "class DetectLanguageResult(BaseModel):\n",
    "    src_lang : str = Field(description=\"Language of the source text\")\n",
    "    src_text : str = Field(description=\"Source text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00485b37-d28d-490d-8fd6-be1f1cac303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMTranslateResult(BaseModel):\n",
    "    src_lang : str = Field(description=\"Language of the source text\")\n",
    "    tgt_text : Optional[str] = Field(description=\"Translated text, or null if already in the desired target language\")\n",
    "\n",
    "class TranslateResult(BaseModel):\n",
    "    src_lang : str = Field(description=\"Language of the source text\")\n",
    "    src_text : str = Field(description=\"Source text\")\n",
    "    tgt_lang : str = Field(description=\"Language of the target text\")\n",
    "    tgt_text : Optional[str] = Field(description=\"Target text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ee38903-e099-4df1-8812-73c8389491f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(ABC):\n",
    "    @abstractmethod\n",
    "    def detect_language(self, src_text : str) -> Tuple[DetectLanguageResult, RequestMetadata]:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def translate(self, src_text : str, tgt_lang : str) -> Tuple[TranslateResult, RequestMetadata]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629952ef-b16c-4f6c-ae67-4e3a3403e937",
   "metadata": {},
   "source": [
    "## Templates & prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5cecbc0-6608-4628-ab0a-31569e35ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_system = (\n",
    "    \"You are an expert translator fluent in many languages. You will be \" \\\n",
    "    \"provided with a source text and a target language.  Your task is to \" \\\n",
    "    \"accurately translate the source text into the target language, while \" \\\n",
    "    \"preserving the original meaning and style as closely as possible. You \" \\\n",
    "    \"will also identify the source language of the text. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cdaca3c-61ca-437d-ab89-1d4770ce5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_prompt = Template(\n",
    "    \"Please translate the following text into ${tgt_lang} and tell me the \" \\\n",
    "    \"language of the original text:\\n\\n\" \\\n",
    "    \"${src_text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "881e0c3e-c776-4481-acf4-d732e794d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_system = (\n",
    "    \"You are an expert linguist specializing in language \" \\\n",
    "    \"identification. You will be provided with a source text. Your task is \" \\\n",
    "    \"to accurately determine the source language of the given text.\" \\\n",
    "    \"Use full language names like 'English', 'Spanish', 'Kazakh', 'Greek', etc.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e54ab9-fea6-4e38-aa35-d46c1213bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_prompt = Template(\n",
    "    \"Determine the source language of the following text:\\n\\n\" \\\n",
    "    \"${src_text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511fb89c-d8f4-4f1f-925f-a425a02059f0",
   "metadata": {},
   "source": [
    "## Test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59fc1262-a18c-4e03-b67d-8c896685bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translator(T):\n",
    "    total_duration = 0\n",
    "    print('*' * 80)\n",
    "    print('T.detect_language(\"Hello, world\")')\n",
    "    res = T.detect_language(\"Hello, world\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "    \n",
    "    print('*' * 80)\n",
    "    print('T.translate(\"Hello, world\", \"Español\")')\n",
    "    res = T.translate(\"Hello, world\", \"Español\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print('*' * 80)\n",
    "    print('T.translate(\"Hello mate oi!\", \"Español\")')\n",
    "    res = T.translate(\"Hello mate oi!\", \"Español\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print('*' * 80)\n",
    "    print('T.translate(\"Hello mate oi!\", \"Español de España\")')\n",
    "    res = T.translate(\"Hello mate oi!\", \"Español de España\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print('*' * 80)\n",
    "    print('T.translate(\"Hello mate oi!\", \"Español de México\")')\n",
    "    res = T.translate(\"Hello mate oi!\", \"Español de México\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print('*' * 80)\n",
    "    print('T.translate(\"Hello mate oi!\", \"Español del norte de México, bien norteño patrón!\")')\n",
    "    res = T.translate(\"Hello mate oi!\", \"Español del norte de México, bien norteño patrón!\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print('*' * 80)\n",
    "    print('T.detect_language(\"Сәлем, әлем!\")')\n",
    "    res = T.detect_language(\"Сәлем, әлем!\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print('*' * 80)\n",
    "    print('T.translate(\"Сәлем, әлем!\", \"Español\")')\n",
    "    res = T.translate(\"Сәлем, әлем!\", \"Español\")\n",
    "    print(res[0])\n",
    "    print(res[1])\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print('*' * 80)\n",
    "    print('Al lector traducido al español')\n",
    "    res = T.translate(\n",
    "        \"\"\"La sottise, l'erreur, le péché, la lésine,\n",
    "    Occupent nos esprits et travaillent nos corps,\n",
    "    Et nous alimentons nos aimables remords,\n",
    "    Comme les mendiants nourrissent leur vermine.\n",
    "    \n",
    "    Nos péchés sont têtus, nos repentirs sont lâches;\n",
    "    Nous nous faisons payer grassement nos aveux,\n",
    "    Et nous rentrons gaiement dans le chemin bourbeux,\n",
    "    Croyant par de vils pleurs laver toutes nos taches.\n",
    "    \n",
    "    Sur l'oreiller du mal c'est Satan Trismégiste\n",
    "    Qui berce longuement notre esprit enchanté,\n",
    "    Et le riche métal de notre volonté\n",
    "    Est tout vaporisé par ce savant chimiste.\n",
    "    \n",
    "    C'est le Diable qui tient les fils qui nous remuent!\n",
    "    Aux objets répugnants nous trouvons des appas;\n",
    "    Chaque jour vers l'Enfer nous descendons d'un pas,\n",
    "    Sans horreur, à travers des ténèbres qui puent.\"\"\",\n",
    "        \"Español\",\n",
    "    )\n",
    "    print(res[0].tgt_text)\n",
    "    total_duration += res[1].duration\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TOTAL DURATION:\", total_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3a8c1-2e10-4c94-8245-f84577acb7e2",
   "metadata": {},
   "source": [
    "## Using Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d0635a4-0365-4529-9b9b-5f63caac784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleTranslator(Translator):\n",
    "    def __init__(self):\n",
    "        self.model = \"gemini-2.0-flash-lite-preview-02-05\"\n",
    "    \n",
    "    def _make_config(self, system, schema):\n",
    "        return clients.google_api.types.GenerateContentConfig(\n",
    "            system_instruction=system,\n",
    "            temperature=0,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "        )\n",
    "\n",
    "    def _make_metadata(self, res, elapsed):\n",
    "        return RequestMetadata(\n",
    "            provider=\"google\",\n",
    "            model=self.model,\n",
    "            duration=elapsed,\n",
    "            input_tokens=res.usage_metadata.prompt_token_count,\n",
    "            output_tokens=res.usage_metadata.candidates_token_count,\n",
    "        )\n",
    "\n",
    "    def _prompt(self, config, prompt):\n",
    "        return clients.google.models.generate_content(\n",
    "            model=self.model,\n",
    "            config=config,\n",
    "            contents=prompt,\n",
    "        )\n",
    "    \n",
    "    def detect_language(self, src_text : str) -> Tuple[DetectLanguageResult, RequestMetadata]:\n",
    "        prompt = detect_prompt.substitute(src_text=src_text)\n",
    "        config = self._make_config(detect_system, LLMDetectLanguageResult)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(config, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        return (\n",
    "            DetectLanguageResult(\n",
    "                src_text=src_text,\n",
    "                src_lang=res.parsed.src_lang,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed),\n",
    "        )\n",
    "\n",
    "    def translate(self, src_text : str, tgt_lang : str) -> Tuple[TranslateResult, RequestMetadata]:\n",
    "        prompt = translate_prompt.substitute(tgt_lang=tgt_lang, src_text=src_text)\n",
    "        config = self._make_config(translate_system, LLMTranslateResult)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(config, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        return (\n",
    "            TranslateResult(\n",
    "                src_lang = res.parsed.src_lang,\n",
    "                src_text = src_text,\n",
    "                tgt_lang = tgt_lang,\n",
    "                tgt_text = res.parsed.tgt_text,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861556b9-cb42-4335-857f-836f1d743aaf",
   "metadata": {},
   "source": [
    "La siguiente celda dura como 9.505784358014353 segundos, descomentala para calar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "395b10e9-3a7b-4a53-a6a8-4a672fbfeed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "T.detect_language(\"Hello, world\")\n",
      "src_lang='English' src_text='Hello, world'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.8479017170002408 input_tokens=78 output_tokens=13\n",
      "********************************************************************************\n",
      "T.translate(\"Hello, world\", \"Español\")\n",
      "src_lang='English' src_text='Hello, world' tgt_lang='Español' tgt_text='Hola, mundo'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.8220592180000494 input_tokens=112 output_tokens=25\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español' tgt_text='¡Hola amigo!'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.8863348080003561 input_tokens=113 output_tokens=24\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de España\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español de España' tgt_text='¡Hola amigo!'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.8443950890000451 input_tokens=115 output_tokens=24\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de México\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español de México' tgt_text='¡Hola amigo!'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.8282265620000544 input_tokens=115 output_tokens=24\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español del norte de México, bien norteño patrón!\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español del norte de México, bien norteño patrón!' tgt_text='¡Hola compa! ¡Quihubo!'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.8738385850001578 input_tokens=123 output_tokens=29\n",
      "********************************************************************************\n",
      "T.detect_language(\"Сәлем, әлем!\")\n",
      "src_lang='Kazakh' src_text='Сәлем, әлем!'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.7551178580001761 input_tokens=82 output_tokens=14\n",
      "********************************************************************************\n",
      "T.translate(\"Сәлем, әлем!\", \"Español\")\n",
      "src_lang='Kazakh' src_text='Сәлем, әлем!' tgt_lang='Español' tgt_text='¡Hola mundo!'\n",
      "provider='google' model='gemini-2.0-flash-lite-preview-02-05' duration=0.8713371680000819 input_tokens=116 output_tokens=25\n",
      "********************************************************************************\n",
      "Al lector traducido al español\n",
      "La tontería, el error, el pecado, la avaricia,\n",
      "    Ocupan nuestras mentes y trabajan nuestros cuerpos,\n",
      "    Y alimentamos nuestros amables remordimientos,\n",
      "    Como los mendigos alimentan su vermina.\n",
      "    \n",
      "    Nuestros pecados son testarudos, nuestros arrepentimientos son cobardes;\n",
      "    Nos hacemos pagar generosamente nuestras confesiones,\n",
      "    Y volvemos alegremente al camino fangoso,\n",
      "    Creyendo que con viles llantos lavaremos todas nuestras manchas.\n",
      "    \n",
      "    Sobre la almohada del mal es Satán Trismegisto\n",
      "    Quien mece largamente nuestro espíritu encantado,\n",
      "    Y el rico metal de nuestra voluntad\n",
      "    Está todo vaporizado por este sabio químico.\n",
      "    \n",
      "    ¡Es el Diablo quien sostiene los hilos que nos mueven!\n",
      "    A los objetos repugnantes les encontramos encantos;\n",
      "    Cada día hacia el Infierno descendemos un paso,\n",
      "    Sin horror, a través de tinieblas que apestan.\n",
      "================================================================================\n",
      "TOTAL DURATION: 9.088178892000997\n"
     ]
    }
   ],
   "source": [
    "google_translator = GoogleTranslator()\n",
    "test_translator(google_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a166c-3323-4e75-a299-aee60d041820",
   "metadata": {},
   "source": [
    "## Using Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3c49033-af53-4f46-b4f3-f2cb4f485e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnthropicTranslator(Translator):\n",
    "    def __init__(self):\n",
    "        self.model = \"claude-3-5-haiku-latest\"\n",
    "        self.max_tokens=1024\n",
    "        self.tools = [\n",
    "            {\n",
    "                \"name\": \"detect-language\",\n",
    "                \"description\": \"Accurately identifies the language of the given text.\",\n",
    "                \"input_schema\": LLMDetectLanguageResult.model_json_schema(),\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"detect-and-translate-language\",\n",
    "                \"description\": \"Identifies the language of the source text and provides an accurate translation to the specified target language.\",\n",
    "                \"input_schema\": LLMTranslateResult.model_json_schema(),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def _make_metadata(self, res, elapsed):\n",
    "        return RequestMetadata(\n",
    "            provider=\"anthropic\",\n",
    "            model=self.model,\n",
    "            duration=elapsed,\n",
    "            input_tokens=res.usage.input_tokens,\n",
    "            output_tokens=res.usage.output_tokens,\n",
    "        )\n",
    "\n",
    "    def _prompt(self, system, tool, prompt):\n",
    "        return clients.anthropic.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=self.max_tokens,\n",
    "            system=system,\n",
    "            temperature=0,\n",
    "            tools=self.tools,\n",
    "            tool_choice={\"type\": \"tool\", \"name\": tool},\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def detect_language(self, src_text : str) -> Tuple[DetectLanguageResult, RequestMetadata]:\n",
    "        prompt = detect_prompt.substitute(src_text=src_text)\n",
    "        tool = \"detect-language\"\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(detect_system, tool, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        metadata = self._make_metadata(res, elapsed)\n",
    "        try:\n",
    "            parsed = LLMDetectLanguageResult.model_validate(res.content[0].input)\n",
    "            return (\n",
    "                DetectLanguageResult(\n",
    "                    src_text=src_text,\n",
    "                    src_lang=parsed.src_lang\n",
    "                ),\n",
    "                self._make_metadata(res, elapsed),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Something went wrong: {e}\")\n",
    "            return res, metadata\n",
    "\n",
    "    def translate(self, src_text : str, tgt_lang : str) -> Tuple[TranslateResult, RequestMetadata]:\n",
    "        prompt = translate_prompt.substitute(tgt_lang=tgt_lang, src_text=src_text)\n",
    "        tool = \"detect-and-translate-language\"\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(translate_system, tool, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        metadata = self._make_metadata(res, elapsed)\n",
    "        try:\n",
    "            parsed = LLMTranslateResult.model_validate(res.content[0].input)\n",
    "            return (\n",
    "                TranslateResult(\n",
    "                    src_lang = parsed.src_lang,\n",
    "                    src_text = src_text,\n",
    "                    tgt_lang = tgt_lang,\n",
    "                    tgt_text = parsed.tgt_text,\n",
    "                ),\n",
    "                self._make_metadata(res, elapsed),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Something went wrong: {e}\")\n",
    "            return res, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7aa0c4-2fc6-4b1f-8808-f367ee907ed1",
   "metadata": {},
   "source": [
    "La siguiente celda tarda como 13.52147050700296 segundos, descomentala para calar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba030691-e171-401b-806d-d012b07625f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "T.detect_language(\"Hello, world\")\n",
      "src_lang='English' src_text='Hello, world'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.018118905999927 input_tokens=697 output_tokens=35\n",
      "********************************************************************************\n",
      "T.translate(\"Hello, world\", \"Español\")\n",
      "src_lang='en' src_text='Hello, world' tgt_lang='Español' tgt_text='Hola, mundo'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.329876556000272 input_tokens=712 output_tokens=59\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español\")\n",
      "src_lang='en' src_text='Hello mate oi!' tgt_lang='Español' tgt_text='¡Hola amigo!'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.5770502209998085 input_tokens=714 output_tokens=61\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de España\")\n",
      "src_lang='en' src_text='Hello mate oi!' tgt_lang='Español de España' tgt_text='¡Hola colega!'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.4401689490000535 input_tokens=716 output_tokens=61\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de México\")\n",
      "src_lang='en' src_text='Hello mate oi!' tgt_lang='Español de México' tgt_text='¡Hola, compa!'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.6083981330002644 input_tokens=716 output_tokens=62\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español del norte de México, bien norteño patrón!\")\n",
      "src_lang='en' src_text='Hello mate oi!' tgt_lang='Español del norte de México, bien norteño patrón!' tgt_text='¡Ey, compa! ¡Qué onda!'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.3613916399999653 input_tokens=727 output_tokens=71\n",
      "********************************************************************************\n",
      "T.detect_language(\"Сәлем, әлем!\")\n",
      "src_lang='Kazakh' src_text='Сәлем, әлем!'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.0114966209998784 input_tokens=704 output_tokens=37\n",
      "********************************************************************************\n",
      "T.translate(\"Сәлем, әлем!\", \"Español\")\n",
      "src_lang='kk' src_text='Сәлем, әлем!' tgt_lang='Español' tgt_text='¡Hola, mundo!'\n",
      "provider='anthropic' model='claude-3-5-haiku-latest' duration=1.4990553729999192 input_tokens=719 output_tokens=62\n",
      "********************************************************************************\n",
      "Al lector traducido al español\n",
      "La estupidez, el error, el pecado, la mezquindad,\n",
      "    Ocupan nuestros espíritus y trabajan nuestros cuerpos,\n",
      "    Y alimentamos nuestros amables remordimientos,\n",
      "    Como los mendigos alimentan su inmundicia.\n",
      "\n",
      "    Nuestros pecados son tercos, nuestros arrepentimientos son cobardes;\n",
      "    Nos hacemos pagar generosamente nuestras confesiones,\n",
      "    Y regresamos alegremente al camino cenagoso,\n",
      "    Creyendo lavar todas nuestras manchas con viles lágrimas.\n",
      "\n",
      "    En la almohada del mal, es Satán Trismegisto\n",
      "    Quien mece largamente nuestro espíritu encantado,\n",
      "    Y el rico metal de nuestra voluntad\n",
      "    Está completamente vaporizado por este sabio químico.\n",
      "\n",
      "    ¡Es el Diablo quien sostiene los hilos que nos mueven!\n",
      "    A los objetos repugnantes les encontramos atractivos;\n",
      "    Cada día descendemos un paso hacia el Infierno,\n",
      "    Sin horror, a través de tinieblas que apestan.\n",
      "================================================================================\n",
      "TOTAL DURATION: 15.19493966500022\n"
     ]
    }
   ],
   "source": [
    "anthropic_translator = AnthropicTranslator()\n",
    "test_translator(anthropic_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aaa125-8f70-4823-aa87-19438db1e188",
   "metadata": {},
   "source": [
    "## Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6c7485-5f00-4e63-9b73-e7d77c234d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAITranslator(Translator):\n",
    "    def __init__(self):\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "\n",
    "    def _make_metadata(self, res, elapsed):\n",
    "        return RequestMetadata(\n",
    "            provider=\"openai\",\n",
    "            model=self.model,\n",
    "            duration=elapsed,\n",
    "            input_tokens=res.usage.prompt_tokens,\n",
    "            output_tokens=res.usage.completion_tokens,\n",
    "        )\n",
    "\n",
    "    def _prompt(self, system, res_format, prompt):\n",
    "        return clients.openai.beta.chat.completions.parse(\n",
    "            model=self.model,\n",
    "            temperature=0,\n",
    "            response_format=res_format,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\",\n",
    "                 \"content\": [{\"type\": \"text\", \"text\": system}]},\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": [{\"type\": \"text\",\"text\": prompt}]},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def detect_language(self, src_text : str) -> Tuple[DetectLanguageResult, RequestMetadata]:\n",
    "        prompt = detect_prompt.substitute(src_text=src_text)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(detect_system, LLMDetectLanguageResult, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        return (\n",
    "            DetectLanguageResult(\n",
    "                src_text=src_text,\n",
    "                src_lang=res.choices[0].message.parsed.src_lang,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed)\n",
    "        )\n",
    "\n",
    "    def translate(self, src_text : str, tgt_lang : str) -> Tuple[TranslateResult, RequestMetadata]:\n",
    "        prompt = translate_prompt.substitute(tgt_lang=tgt_lang, src_text=src_text)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(translate_system, LLMTranslateResult, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        return (\n",
    "            TranslateResult(\n",
    "                src_lang = res.choices[0].message.parsed.src_lang,\n",
    "                src_text = src_text,\n",
    "                tgt_lang = tgt_lang,\n",
    "                tgt_text = res.choices[0].message.parsed.tgt_text,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136faf5-28cb-4370-8013-5daeaab5dc08",
   "metadata": {},
   "source": [
    "La siguiente celda tarda como 9.407515291000891 segundos, descomentala para calar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "112fded8-a15b-42b0-9178-c82441596c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "T.detect_language(\"Hello, world\")\n",
      "src_lang='English' src_text='Hello, world'\n",
      "provider='openai' model='gpt-4o-mini' duration=1.1554771430000983 input_tokens=130 output_tokens=7\n",
      "********************************************************************************\n",
      "T.translate(\"Hello, world\", \"Español\")\n",
      "src_lang='English' src_text='Hello, world' tgt_lang='Español' tgt_text='Hola, mundo'\n",
      "provider='openai' model='gpt-4o-mini' duration=1.3574547289999828 input_tokens=183 output_tokens=15\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español' tgt_text='¡Hola amigo!'\n",
      "provider='openai' model='gpt-4o-mini' duration=0.5820824110001013 input_tokens=184 output_tokens=16\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de España\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español de España' tgt_text='¡Hola, colega!'\n",
      "provider='openai' model='gpt-4o-mini' duration=0.5912490050000088 input_tokens=186 output_tokens=17\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de México\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español de México' tgt_text='¡Hola amigo!'\n",
      "provider='openai' model='gpt-4o-mini' duration=3.4955087919997823 input_tokens=186 output_tokens=16\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español del norte de México, bien norteño patrón!\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español del norte de México, bien norteño patrón!' tgt_text='¡Hola compadre!'\n",
      "provider='openai' model='gpt-4o-mini' duration=0.6867602690003878 input_tokens=194 output_tokens=17\n",
      "********************************************************************************\n",
      "T.detect_language(\"Сәлем, әлем!\")\n",
      "src_lang='Kazakh' src_text='Сәлем, әлем!'\n",
      "provider='openai' model='gpt-4o-mini' duration=0.4233319009999832 input_tokens=133 output_tokens=8\n",
      "********************************************************************************\n",
      "T.translate(\"Сәлем, әлем!\", \"Español\")\n",
      "src_lang='Kazakh' src_text='Сәлем, әлем!' tgt_lang='Español' tgt_text='¡Hola, mundo!'\n",
      "provider='openai' model='gpt-4o-mini' duration=0.8371541189999334 input_tokens=186 output_tokens=18\n",
      "********************************************************************************\n",
      "Al lector traducido al español\n",
      "La necedad, el error, el pecado, la avaricia,\n",
      "    Ocupan nuestras mentes y trabajan nuestros cuerpos,\n",
      "    Y alimentamos nuestros amables remordimientos,\n",
      "    Como los mendigos alimentan su alimaña.\n",
      "    \n",
      "    Nuestros pecados son tercos, nuestros arrepentimientos son cobardes;\n",
      "    Nos hacemos pagar generosamente nuestras confesiones,\n",
      "    Y regresamos alegremente al camino fangoso,\n",
      "    Creyendo que con vil llanto lavamos todas nuestras manchas.\n",
      "    \n",
      "    Sobre la almohada del mal está Satanás Trismegisto\n",
      "    Quien mece largamente nuestra mente encantada,\n",
      "    Y el rico metal de nuestra voluntad\n",
      "    Se evapora por completo por este sabio químico.\n",
      "    \n",
      "    ¡Es el Diablo quien sostiene los hilos que nos mueven!\n",
      "    A los objetos repugnantes les encontramos atractivos;\n",
      "    Cada día hacia el Infierno descendemos un paso,\n",
      "    Sin horror, a través de tinieblas que apestan.\n",
      "================================================================================\n",
      "TOTAL DURATION: 12.685684165000112\n"
     ]
    }
   ],
   "source": [
    "openai_translator = OpenAITranslator()\n",
    "test_translator(openai_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e830ee-cd06-42b2-97c4-9be4319d4f61",
   "metadata": {},
   "source": [
    "## Using Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29db22e2-211c-4afc-90da-c23c0d18200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralTranslator(Translator):\n",
    "    def __init__(self):\n",
    "        self.model = \"mistral-small-latest\"\n",
    "\n",
    "    def _make_metadata(self, res, elapsed):\n",
    "        return RequestMetadata(\n",
    "            provider=\"mistral\",\n",
    "            model=self.model,\n",
    "            duration=elapsed,\n",
    "            input_tokens=res.usage.prompt_tokens,\n",
    "            output_tokens=res.usage.completion_tokens,\n",
    "        )\n",
    "\n",
    "    def _prompt(self, system, res_format, prompt):\n",
    "        return clients.mistral.chat.parse(\n",
    "            model=self.model,\n",
    "            temperature=0,\n",
    "            response_format=res_format,\n",
    "            messages=[\n",
    "                {\"role\": \"system\",\n",
    "                 \"content\": system},\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def detect_language(self, src_text : str) -> Tuple[DetectLanguageResult, RequestMetadata]:\n",
    "        prompt = detect_prompt.substitute(src_text=src_text)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(detect_system, LLMDetectLanguageResult, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        return (\n",
    "            DetectLanguageResult(\n",
    "                src_text=src_text,\n",
    "                src_lang=res.choices[0].message.parsed.src_lang,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed)\n",
    "        )\n",
    "\n",
    "    def translate(self, src_text : str, tgt_lang : str) -> Tuple[TranslateResult, RequestMetadata]:\n",
    "        prompt = translate_prompt.substitute(tgt_lang=tgt_lang, src_text=src_text)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(translate_system, LLMTranslateResult, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        return (\n",
    "            TranslateResult(\n",
    "                src_lang = res.choices[0].message.parsed.src_lang,\n",
    "                src_text = src_text,\n",
    "                tgt_lang = tgt_lang,\n",
    "                tgt_text = res.choices[0].message.parsed.tgt_text,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a7175-e780-4995-bfe6-794f43173d4a",
   "metadata": {},
   "source": [
    "La siguiente celda tarda como 12.31103517000156 segundos, descomentala para calar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3adce28e-5608-4432-ab8f-57c4d988ef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "T.detect_language(\"Hello, world\")\n",
      "src_lang='English' src_text='Hello, world'\n",
      "provider='mistral' model='mistral-small-latest' duration=0.9361330310002813 input_tokens=74 output_tokens=7\n",
      "********************************************************************************\n",
      "T.translate(\"Hello, world\", \"Español\")\n",
      "src_lang='English' src_text='Hello, world' tgt_lang='Español' tgt_text='Hola, mundo'\n",
      "provider='mistral' model='mistral-small-latest' duration=1.9519102040003418 input_tokens=86 output_tokens=22\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español\")\n"
     ]
    },
    {
     "ename": "SDKError",
     "evalue": "API error occurred: Status 502\n<html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>cloudflare</center>\r\n</body>\r\n</html>\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mistral_translator \u001b[38;5;241m=\u001b[39m MistralTranslator()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_translator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmistral_translator\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m, in \u001b[0;36mtest_translator\u001b[0;34m(T)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT.translate(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello mate oi!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEspañol\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello mate oi!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEspañol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[24], line 44\u001b[0m, in \u001b[0;36mMistralTranslator.translate\u001b[0;34m(self, src_text, tgt_lang)\u001b[0m\n\u001b[1;32m     42\u001b[0m prompt \u001b[38;5;241m=\u001b[39m translate_prompt\u001b[38;5;241m.\u001b[39msubstitute(tgt_lang\u001b[38;5;241m=\u001b[39mtgt_lang, src_text\u001b[38;5;241m=\u001b[39msrc_text)\n\u001b[1;32m     43\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 44\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate_system\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLLMTranslateResult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m, in \u001b[0;36mMistralTranslator._prompt\u001b[0;34m(self, system, res_format, prompt)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, system, res_format, prompt):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmistral\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/areas/repos/elelem/.venv/lib/python3.13/site-packages/mistralai/chat.py:39\u001b[0m, in \u001b[0;36mChat.parse\u001b[0;34m(self, response_format, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m json_response_format \u001b[38;5;241m=\u001b[39m response_format_from_pydantic_model(response_format)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Run the inference\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_response_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Parse response back to the input pydantic model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m parsed_response \u001b[38;5;241m=\u001b[39m convert_to_parsed_chat_completion_response(\n\u001b[1;32m     42\u001b[0m     response, response_format\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/areas/repos/elelem/.venv/lib/python3.13/site-packages/mistralai/chat.py:240\u001b[0m, in \u001b[0;36mChat.complete\u001b[0;34m(self, model, messages, temperature, top_p, max_tokens, stream, stop, random_seed, response_format, tools, tool_choice, presence_penalty, frequency_penalty, n, prediction, safe_prompt, retries, server_url, timeout_ms, http_headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    239\u001b[0m     http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res_text, http_res\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    244\u001b[0m content_type \u001b[38;5;241m=\u001b[39m http_res\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 502\n<html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>cloudflare</center>\r\n</body>\r\n</html>\r\n"
     ]
    }
   ],
   "source": [
    "mistral_translator = MistralTranslator()\n",
    "test_translator(mistral_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8cbbc-a9fe-4821-83fa-3563d2a03c37",
   "metadata": {},
   "source": [
    "## Using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f2fc600-dbca-4958-a190-f7781ccd753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaTranslator(Translator):\n",
    "    def __init__(self):\n",
    "        self.model = \"gemma2:9b\"\n",
    "\n",
    "    def _make_metadata(self, res, elapsed):\n",
    "        return RequestMetadata(\n",
    "            provider=\"ollama\",\n",
    "            model=self.model,\n",
    "            duration=elapsed,\n",
    "            input_tokens=res.prompt_eval_count,\n",
    "            output_tokens=res.eval_count,\n",
    "        )\n",
    "\n",
    "    def _prompt(self, system, res_format, prompt):\n",
    "        return clients.ollama.generate(\n",
    "            model=self.model,\n",
    "            system=system,\n",
    "            format=res_format.model_json_schema(),\n",
    "            options={\n",
    "                \"temperature\": 0,\n",
    "            },\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "    def detect_language(self, src_text : str) -> Tuple[DetectLanguageResult, RequestMetadata]:\n",
    "        prompt = detect_prompt.substitute(src_text=src_text)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(detect_system, LLMDetectLanguageResult, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        parsed = LLMDetectLanguageResult.model_validate_json(res.response)\n",
    "        return (\n",
    "            DetectLanguageResult(\n",
    "                src_text=src_text,\n",
    "                src_lang=parsed.src_lang,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed)\n",
    "        )\n",
    "\n",
    "    def translate(self, src_text : str, tgt_lang : str) -> Tuple[TranslateResult, RequestMetadata]:\n",
    "        prompt = translate_prompt.substitute(tgt_lang=tgt_lang, src_text=src_text)\n",
    "        start_time = time.perf_counter()\n",
    "        res = self._prompt(translate_system, LLMTranslateResult, prompt)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        parsed = LLMTranslateResult.model_validate_json(res.response)\n",
    "        return (\n",
    "            TranslateResult(\n",
    "                src_lang = parsed.src_lang,\n",
    "                src_text = src_text,\n",
    "                tgt_lang = tgt_lang,\n",
    "                tgt_text = parsed.tgt_text,\n",
    "            ),\n",
    "            self._make_metadata(res, elapsed)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3609734a-c38f-4e5a-87e6-de75fb889d9a",
   "metadata": {},
   "source": [
    "La siguiente celda tarda como:\n",
    "- 75.49605615798646 segundos en la violenta\n",
    "- 49.216149886000494 segundos en la furiosa\n",
    "- ??? segundos en la pitaya\n",
    "\n",
    "descomentala para calar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a5918b8-cd0a-492a-bd5c-5d9ae4c0670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "T.detect_language(\"Hello, world\")\n",
      "src_lang='English' src_text='Hello, world'\n",
      "provider='ollama' model='gemma2:9b' duration=3.9903903190001984 input_tokens=76 output_tokens=13\n",
      "********************************************************************************\n",
      "T.translate(\"Hello, world\", \"Español\")\n",
      "src_lang='English' src_text='Hello, world' tgt_lang='Español' tgt_text='¡Hola mundo!'\n",
      "provider='ollama' model='gemma2:9b' duration=2.9429496449997714 input_tokens=92 output_tokens=21\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español' tgt_text='¡Hola amigo, oi!'\n",
      "provider='ollama' model='gemma2:9b' duration=2.6775158430000374 input_tokens=93 output_tokens=25\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de España\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español de España' tgt_text='¡Hola colega, ¡oi!'\n",
      "provider='ollama' model='gemma2:9b' duration=2.95796704400027 input_tokens=95 output_tokens=26\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español de México\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español de México' tgt_text='¡Hola compa! ¡Oi! '\n",
      "provider='ollama' model='gemma2:9b' duration=2.6755861130000085 input_tokens=95 output_tokens=24\n",
      "********************************************************************************\n",
      "T.translate(\"Hello mate oi!\", \"Español del norte de México, bien norteño patrón!\")\n",
      "src_lang='English' src_text='Hello mate oi!' tgt_lang='Español del norte de México, bien norteño patrón!' tgt_text='¡Hola compa, ¡oi!'\n",
      "provider='ollama' model='gemma2:9b' duration=2.9830154679998486 input_tokens=103 output_tokens=26\n",
      "********************************************************************************\n",
      "T.detect_language(\"Сәлем, әлем!\")\n",
      "src_lang='Kazakh' src_text='Сәлем, әлем!'\n",
      "provider='ollama' model='gemma2:9b' duration=2.304632755000057 input_tokens=80 output_tokens=16\n",
      "********************************************************************************\n",
      "T.translate(\"Сәлем, әлем!\", \"Español\")\n",
      "src_lang='Kazakh' src_text='Сәлем, әлем!' tgt_lang='Español' tgt_text='¡Hola mundo!'\n",
      "provider='ollama' model='gemma2:9b' duration=3.3078221829996437 input_tokens=96 output_tokens=24\n",
      "********************************************************************************\n",
      "Al lector traducido al español\n",
      "La insensatez, el error, el pecado, la avaricia,\n",
      "    Ocupan nuestros espíritus y trabajan nuestros cuerpos,\n",
      "    Y alimentamos nuestros queridos remordimientos,\n",
      "    Como los mendigos alimentan a sus bichos.\n",
      "    \n",
      "    Nuestros pecados son tercos, nuestros arrepentimientos son cobardes;\n",
      "    Nos hacemos pagar con creces por nuestros confesiones,\n",
      "    Y volvemos alegremente al camino fangoso,\n",
      "    Creyendo que con miserables lágrimas lavamos todas nuestras manchas.\n",
      "    \n",
      "    Sobre el cojín del mal es Satanás Trismegisto\n",
      "    Quien mece largamente nuestro espíritu encantado,\n",
      "    Y el rico metal de nuestra voluntad\n",
      "    Es todo vaporizado por este sabio químico.\n",
      "    \n",
      "    ¡Es el Diablo quien tiene los hilos que nos mueven!\n",
      "    A los objetos repugnantes encontramos tentaciones;\n",
      "    Cada día hacia el Infierno bajamos un paso,\n",
      "    Sin horror, a través de tinieblas que apestan.\n",
      "================================================================================\n",
      "TOTAL DURATION: 50.93084528400004\n"
     ]
    }
   ],
   "source": [
    "ollama_translator = OllamaTranslator()\n",
    "test_translator(ollama_translator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
